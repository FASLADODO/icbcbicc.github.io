---
layout: post
title: "Machine Learning (Zhihua Zhou) Notes 01"
subtitle:   "读书笔记"
author:     "icbcbicc"
header-img: "img/post-bg-08.jpg"
tags: ["machine learning"]
---

# 周志华《机器学习》笔记-01

## 2. 模型的评估与选择

### 2.1 经验误差与过拟合

- **三种误差**

	- **训练误差(training error)、经验误差(empirical error)**：训练集上的误差，受到欠拟合与过拟合的影响

	- **测试误差(testing error)**：测试集上的误差，通过常作为泛化误差的近似

	- **泛化误差(generalization error)**：在新样本上的误差，无法直接获得

- **欠拟合与过拟合**

	- 欠拟合易于克服：增加训练轮数

	- 过拟合是机器学习面临的关键障碍，无法彻底避免。因为机器学习面临的问题通常是$NP$难甚至更难。然而学习算法必然要在多项式时间完成，要想彻底避免过拟合，就要通过误差最小化直接获得最优解，也就意味着$P=NP$。只要坚信$P\ne NP$，过拟合就不可避免。

### 2.2 产生测试集的方法

**2.2.1 留出法(hold-out)**

- 方法：将数据集分为互斥的两个集合。采用分层采样(stratified sampling)来保证2个集合的数据分布一致性（也就是训练集和测试集中相同类型样本所占的比例相同）。

- 缺陷：
	- 由于单次使用留出法得到的结果不够可靠（即使保证相同类别样本比例相同，仍然有多种方式划分），一般采用多次随机划分得到多组{训练集，测试集}进行训练，误差取平均的方式。

	- 训练集占的多：模型精确，但评估结果不够稳定准确；测试集占的多，模型精度不足。此问题没有完美解决方案，一般选取$\frac{2}{3}$到$\frac{4}{5}$的样本用于训练。

**2.2.2 交叉验证法(cross validation)**

- 方法：
	1. 先将数据划为$k$个大小相同，数据分布相同的互斥子集
	2. 每次使用$k-1$个子集进行训练，$1$个做测试。一共可获得$k$组{训练集，测试集}的组合。
	3. 
- 关键：结果的稳定性和真实性很大程度上取决于$k$的选择，因此这个方法被称作“$k$折交叉验证($k$-fold cross validation)”

- 缺陷：与留出法相似，将数据划分为$k$组有很多方式，产生了不确定因素。一般采用进行$p$次划分取均值的方法，也就是“$p$次$k$折交叉验证”，一共得到$k \times p$组{训练集，测试集}。

- 特例：当$k=样本数$，称为“留一法(Leave-One-Out，LOO)”。此方式不受样本划分的影响，评估较准确，但计算量太大。

**2.2.3 自助法(bootstrapping)**

以自助采样法(bootstrapping sampling，亦称“可重复采样/有放回采样”)为基础，**适合数据集小的情况** 。

- 方法：在数据集$D$中有放回的随机取出一个样本，重复$m$次,得到$D'$作为训练集，$D-D'$作为测试集。$D'$中可能有重复的样本。

- 当$m$足够大时，一个样本在$m$次选取中不被选中的概率是
$$$\lim_{m \to \inf}(1-\frac{1}{m})^m \to \frac{1}{e} \approx 0.368$$$
因此$D$中有$36.8\%$的样本不在$D'$中。

- 缺陷：改变了初始数据的分布。数据量足够时一般不采用此方法。

**2.2.4 调参与最终模型**

-  两类参数：
	1. 超参数：人工设定算法的参数，数目较少
	2. 模型参数：由算法生成的参数，数量可以很多

- 在模型最优化后，应该使用所有数据（包括测试集）再训练一次，得到最终模型。

### 2.3 性能度量

**2.3.2 查准率，查全率与$F$度量**

- 混淆矩阵(confusion matrix)：

    |        |    |预测结果||
    |------- |----|---------|
    |		 |    |正例|反例|
    |真实情况|正例|TP  |FN  |
    |        |反例|FP  |TN  |

- 查准率(precision，亦称准确率)：$P = \frac{TP}{TP+FP}$

- 查全率(recall，亦称召回率)：$R = \frac{TP}{TP+FN}$

- 查全率与查准率通常是矛盾的

- P-R曲线
	- 方法：
		1. 将测试样本按概率从大到小排序
		2.  将前$n$个样本作为正例，得到查准率与查全率
		3. $n = n+1$

	- 利用P-R曲线的评估：
	![P-R curve](/img/13.png)
	1. 曲线下面积越大，则该模型越好。当一个曲线包住了另一个曲线而没有相交，则认为外面的曲线表示的模型更好（比如下图中的蓝色比绿色好）。
	2. $P = R$的点称为平衡点(Break-Even Point, BEP)，也就是曲线与对角线的交点，也可作为评估依据：越大越到。

- $F_1$与$F_\beta$度量

	- $$F1 = \frac{2 \times P \times R}{P+R}$$
	实际上就是P与R的调和平均。

	- $$F_\beta = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}$$
	实际上就是P与R的加权调和平均。$\beta$决定了查全率和查准率谁更重要：$\beta > 1$时，查全率更重要；$0 < \beta < 1$时，查准率更重要。

- 宏查准率，宏查全率，宏$F1$；微查准率，微查全率，微$F1$：略

**2.3.3 ROC与AUC**

- 截断点(cut point)的选择：

	将样本按概率从大到小排序

	- 查准率高：截断点靠前

	- 查全率高：截断点靠后

- ROC(Receiver Operating Characteristic，受试者工作特征)
	![ROC curve](/img/14.png)
    - 纵轴：真正率
    $$TPR = \frac{TP}{TP+FN}$$

    - 横轴：假正率
    $$FPR = \frac{FP}{TN+FP}$$

	- 对角线：随机猜测模型

	- 评估方式：与P-R曲线相同。曲线下的面积称为AUC（Aera under ROC）

**2.3.4 代价敏感错误率与代价曲线**

之前的评价方式隐式假设了均等代价，即将错误个数作为代价。在非均等代价下，ROC曲线不能反映出总体代价，此时应该使用代价曲线。

- **代价曲线(cost surve)**
	![cost curve](/img/15.png)
    - 横轴：正例概率代价
        $$P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01}+(1-p) \times cost_{10}}$$
其中，$p$为正例的概率；$cost_{01}$为假反率代价；$cost_{10}$为假正率代价。

    - 纵轴：归一化代价
    $$cost_{norm} = \frac{(1-TPR) \times p \times cost_{01}+FPR \times (1-p) \times cost_{10}}{p \times cost_{01}+(1-p) \times cost_{10}}$$

    - 在ROC曲线上取一点(FPR, TPR)，在代价曲线上将左纵轴上的(0,FPR)与右纵轴上的(1，1-TPR)2点连成线段。所有线段围成的部分面积为**期望总体代价**。

### 2.4 比较检验
用于度量模型在测试集上的性能有多大概率是泛化性能，暂略

**2.4.1 假设检验**
**2.4.2 价差验证t检验**
**2.4.3 McNemar检验**
**2.4.4 Friedman检验与Nemenyi后续检验**

### 2.5 偏差与方差

- **基于均方误差的回归任务的泛化误差可分解为偏差、方差、噪声之和**。

    - 偏差：标记与真实类别的偏差。表明了在当前任务上任何学习算法所能达到的期望泛化误差的最小值，是任务本身的属性。

    - 方差：同样大小的训练集变化所导致的性能变化，即数据扰动造成的影响。

    - 偏差：算法本身的拟合能力。

- 偏差-方差窘境(bias-variance dilemma)

	偏差与方差一般都是冲突的。

	- 训练刚开始：拟合能力不足，受数据扰动的影响较小。偏差主导了泛化错误率。

	- 训练充足：拟合能力很强，受数据扰动的影响大。方差主导了泛化错误率。

- Attention：对于分类任务，由于0/1损失函数的跳变性，偏差-方差分解很困难。