---
layout: post
title: "嘻嘻"
subtitle:  "笔记"
author: "icbcbicc"
header-img: "img/gray.png"
---

## 1. Abstract

In order to overcome large modality difference between optical and thermal infrared facial images, we investigate the problem of transferring thermal infrared facial images into visible ones using conditional adversarial networks(cGANs). The optical faces generated by our method are natural and realistic for human observers and can also be directly used as input of many off-the-shelf face recognition algorithms.

## 2. Introduction

Recent years the use of thermal infrared cameras in the area of security has increased a lot. However, due to the huge modality difference between optical and thermal images, many popular face detection or recognition algorithms will lead to unpleasant results if implemented on thermal facial images. Although many effort has been made on thermal face recognition in the literature, they usually need  task specific preprocess methods. Compared to optical face, the number of research on thermal face is much fewer and less effective. As a result, the utilities of the thermal facial images are constrained.

We propose a transform framework which make thermal facial image much more useful. We transfer thermal infrared facial images into visible ones using conditional adversarial networks. The generated optical faces can be directly used as input of many off-the-shelf face recognition algorithms. As there is no similar work have been done in the literature, our method build a connection between the 2 total different modalities.

We mainly made 2 contributions, the first one is to present a effective framework to convert thermal image into optical one. Another contribution is that we make a database containing precisely aligned thermal and optical facial images.

## 3. Related work

- Infrared
    - thermal face recognition
    - near infrared to optical

- Generative model
    - cGANs
    - image-to-image transfer

## 4. Method

Our method is based on \cite{isola2016image} which uses cGANs to handle image-to-image translation problem. cGANs is composed of 2 main parts: the generator $G$ and discriminator $D$. The generator $G$ tries to learn a mapping from random noise $z$ and input image $x$ to target image $y$, which can be formulated as $x, z \rightarrow y$. The discriminator $D$ is trained to try its best to distinguish the image generated by the generator $G$ from target image $y$. In our case, the input is thermal infrared image and the target is optical image. We follow the conventional optimization process which updates the generator $G$ and discriminator $D$ alternately in each iteration.

### 4.1 The objective function

The objective function can be expressed as

$$G^* = arg \, \min \limits_{G} \, \max \limits_{D} L_{cGAN}(G, D)$$

where

$$L_{cGAN}(G, D) = \mathbb{E}_{x, \, y \backsim \mathit{p}_{data}(x, y)}[log \, D(x, y)] +  \mathbb{E}_{x \backsim \mathit{p}_{data}(x, y), \, z \backsim \mathit{p}_z(z)}[log(1-D(x, G(x, z)))]$$

As illustrated in \cite{isola2016image}, directly deploy this objective function will create artifacts that do not exist in the target image in the output image because cGANs will generate too much high frequency textures. As a solution, adding a $L1$ loss to the objective function can inhibit the generation of excessive high frequency pattern and maintain more low frequency structure. So the final objective function can be formulated as

$$G^* = arg \, \min \limits_{G} \, \max \limits_{D}  L_{cGAN}(G, D) + \lambda \|(y - G(x, z))\| _1$$

where $\lambda$ is a parameter balancing the $L_{cGAN}$ and $L1$ loss.


### 4.2 The generator

We use U-net \cite{ronneberger2015u} as our generator. Different from the traditional autoencoder-decoder structure, U-net adds shortcuts between the $i_{th}$ encoder layer and the $i_{th}$ decoder layer. These skip connections can directly pass information from encoder to decoder without losing important low level information. The architecture of U-net is illustrated in Fig 1.

![u-net](/img/61.jpg)

### 4.3 The discriminator

The goal of the discriminator is to distinguish the fake images generated by the generator from true one. We evaluate the difference between the fake image and the target based on patch difference rather than pixel difference. Usually, using pixel difference(1x1 patch) will leads to blurry result. Applying a big patch as the size of 256x256 add minor improvement to the result compare to a 70x70 patch, but will also make the network too hard to train. To keep a balance between time consumption and performance, we use the patch size of 70x70.

### 4.4 The detector

To further improve the performance, we add a detector in the architecture locating 68 landmarks of face

## 5. Experiments

### 5.1 Data acquisition

Most of the dataset which contains both optical and thermal facial images are poorly aligned, make it very hard to use in many tasks especially related to image-to-image transferring and face alignment. So the dataset utilized in this work is made by ourselves. With our precisely aligned dataset, we are able to acquire accurate facial landmarks of the thermal face through locating the landmarks of its corresponding optical image and then directly mapping these points to the thermal face.

We use a PointGery camera to capture color optical images and a FLIR AX5 camera to obtain thermal images of 30 individuals. For each person we acquire 24 images from different angles. During the whole process, the 2 cameras are unmoved. The raw size of the optical and thermal image are 992x794 and 320x256, respectively. We use the OpenFace \cite{baltruvsaitis2016openface} toolkit to detect face and locate 68 facial landmarks on the optical images and then take these bounding boxes and landmarks as the ground truth of the thermal faces. After detection and alignment, we resize both optical and thermal facial images to 90x90 as our input.

### 5.2 Direct generation

In this section, we directly use the cropped optical facial images as our input and trains the cGANs to generate optical images given optical facial images as ground truth.

### 5.3 Improving the appearance of eyes

The result of the direct generation are already able to tell the difference between individuals and looks pretty similar to real people, but some improvement are still needed to make the appearance of eyes looks more natural and authentic. GANs usually generate very unrestrained images. Now with the help of cGANs, the condition information(in our case, the thermal images) make the output much stable, but we still find some stochasticity in some of the output images, especially around the areas of eyes. This disadvantage is mainly caused by the insufficient information in the eyes areas of thermal images. These areas are usually white and lack important textures describing the contour of eyes. Without enough conditional signal, cGANs can only make up random textures in these areas and hence makes the degree of similarity between output and ground truth to drop.

- Method 1

    To make the eyes more natural, we firstly locate the position of eyes and then reconstruct them. Based on the recent face alignment method \cite{deng2017effective}, we train a single CNN to detect 6 landmarks for each eye and extract the eyes areas. After this, we put these thermal eye patches to another cGANs to generate optical eye patches. Although the thermal patches still lack fundamental information, cGANs only need handling the generation of eyes rather than the whole face, which make the optical eyes looks much better.

- Method 2

    We iteratively localize eye landmarks and generate optical eyes. The more precisely the landmarks are, the more accurate condition information will be feed into the cGANs, hence generate better eyes. With more real eyes, it will be more easy for the CNN to localize



### 5.4 Face re-identification

## Memo

- GAN直接生成 + 专用GAN生成眼睛
- 根据其他特征点预测眼睛特征
