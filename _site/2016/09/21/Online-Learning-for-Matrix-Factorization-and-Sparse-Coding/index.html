<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Write your site description here. It will be used as your sites meta description as well!">

    <title>Online Learning for Matrix Factorization and Sparse Coding - Jam's Blog</title>

    <link rel="canonical" href="http://icbcbicc.github.io/2016/09/21/Online-Learning-for-Matrix-Factorization-and-Sparse-Coding/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <link type="application/atom+xml" rel="alternate" href="http://icbcbicc.github.io/feed.xml" title="Jam's Blog" />

</head>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Jam's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/">Home</a>
                </li>
                
				
                <li>
                    <a href="/about/">About Me</a>
                </li>
				
                
				
                <li>
                    <a href="/archives/">archives</a>
                </li>
				
                
				
                <li>
                    <a href="/contact/">Contact</a>
                </li>
				
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Post Header -->
<header class="intro-header" style="background-image: url('/img/post-bg-20.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Online Learning for Matrix Factorization and Sparse Coding</h1>
                    
                    <h2 class="subheading">论文笔记-2001-JMLR-1339</h2>
                    
                    <span class="meta">Posted by icbcbicc on September 21, 2016</span>
                </div>
            </div>
        </div>
    </div>
</header>
<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

				<h1 id="online-learning-for-matrix-factorization-and-sparse-coding">Online Learning for Matrix Factorization and Sparse Coding</h1>
<p>Authors: <em>Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro</em><br />
<a href="http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf">PDF</a></p>

<p><br /></p>

<p><img src="/img/b5dfacb7-b178-4002-823e-fc5dcfd2d641.png" alt="mindmap" /></p>

<p><br /></p>

<h2 id="introduction">1. Introduction</h2>

<p><br /></p>

<p>Linear decompositon of a matrix using learned dictionary instead of pre-defined one</p>

<p><strong><em>Usage:</em></strong></p>

<ul>
  <li>
    <p>low-level: image denoising, texture synthesis, audio processing</p>
  </li>
  <li>
    <p>high-level: image classification</p>
  </li>
</ul>

<p><strong><em>Differences between maxtrix factorization and PCA:</em></strong></p>

<ul>
  <li>
    <p>It doesn’t impose that the basis vector to be orthogonal</p>
  </li>
  <li>
    <p>Allowing more flexibility to adapt the representation to the data</p>
  </li>
</ul>

<p><strong><em>Variants matrix factorization problem:</em></strong></p>

<ul>
  <li>
    <p>Non-negtive matrix factorization</p>
  </li>
  <li>
    <p>Sparse PCA</p>
  </li>
</ul>

<p><br /></p>

<h2 id="problem-statement">2. Problem Statement</h2>

<ul>
  <li>
    <p>Optimize the <strong><em>empirical cost function:</em></strong></p>

    <script type="math/tex; mode=display">f_n(D) = \frac{1}{n}\sum_{i=1}^n l (x_i,D)</script>

    <ul>
      <li>
        <p>$D\in R^{m * k}$ :dictionary</p>
      </li>
      <li>
        <p>$m &lt; &lt; n$: feature nums less than sample nums</p>
      </li>
      <li>
        <p>$k &lt; &lt; n$: atom nums of dictoinary less than sample nums</p>
      </li>
      <li>
        <p>$k &gt; m$: <strong><em>Overcomplete dictionary</em></strong>: atoms more than feature nums</p>
      </li>
    </ul>
  </li>
  <li>
    <p>$l_1$ sparse coding problem (<strong><em>basis pursuit</em></strong>):</p>

    <script type="math/tex; mode=display">l_1(x,D) = min\frac{1}{2}{\|x-D\alpha\|}_2^2+\lambda\|\alpha\|_1 \quad s.t. \quad \alpha \in R^k</script>

    <ul>
      <li>
        <p><strong>Disadvantage:</strong></p>

        <p>There is no direct analytic link between the value of $\lambda$ and the corresponding effective sparsity $|\alpha|_0$</p>
      </li>
      <li>
        <p><strong>Solution:</strong></p>

        <p>To prevent $D$ from having arbitrarily large values (which would lead to arbitrarily small values of $\alpha$), it is common to <strong><em>constrain its columns $d_1, . . . ,d_k$ to have an $l_2-norm$</em></strong> less than or equal to 1</p>

        <script type="math/tex; mode=display">C = \{D \in R^{m * k}\quad s.t. \quad \forall j=1, ... ,k \quad d_j^Td_j \le 1\}</script>

        <p>It’s a joint optimization problem with respet to $D$ and $\alpha=[\alpha_1, …, \alpha_k] \in R^{k * n}$, which is <strong><em>not jointly convex</em></strong> but <strong><em>convex with respect to each of the two variables</em></strong> $D$ and $\alpha$ when the other one is fixed</p>
      </li>
      <li>
        <p><strong>Optimization method:</strong></p>

        <ul>
          <li>
            <p><strong><em>Alternate between the two variables</em></strong>, minimizing over one while keeping the other one fixed</p>
          </li>
          <li>
            <p>Since the computation of $\alpha$ dominates the cost of each iteration in this <strong><em>block-coordinate descent</em></strong> approach, a <strong><em>second-order optimization</em></strong> technique can be used to accurately estimate $D$ at each step when $\alpha$ is fixed</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong><em>Stochastic gradient</em></strong>, whose rate of convergence is very poor in conventional optimization terms, may in fact in certain settings <strong><em>to be faster in reaching a solution with low expected cost than second-order batch</em></strong></p>
      </li>
      <li>
        <p><strong>Dictionary learning:</strong></p>

        <p>The classical <strong><em>projected first-order projected stochastic gradient descent</em></strong> consists of a sequence of updates of $D$</p>

        <script type="math/tex; mode=display">D_t = \prod [{D_{t-1}-\delta_t \nabla_C l(x_t,D_{t-1})}]</script>

        <ol>
          <li>
            <p>$\nabla_C: \quad$ Orthogonal projector onto $C$</p>
          </li>
          <li>
            <p>$x_t: \quad$ i.i.d samples obtained by cycling on a randomly permuted training set</p>
          </li>
          <li>
            <p>$\delta_t: \quad$ learning rate(good results are obtained using  $\delta_t = \frac{a}{t+b}$ where $a$ and $b$ are chosen depended on the training data)</p>
          </li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="online-dictionary-learning">3. Online Dictionary Learning</h2>

<p><br /></p>

<h4 id="algorithm-outline">3.1 Algorithm Outline</h4>

<ul>
  <li>
    <p>Assuming that the training set is composed of <strong><em>i.i.d. samples</em></strong></p>
  </li>
  <li>
    <script type="math/tex; mode=display">f_t(D) = \frac{1}{t}\sum_{i=1}^t l (x_i,D)</script>

    <p>and</p>

    <script type="math/tex; mode=display">\hat{f_t}(D) = \frac{1}{t}\sum_{i=1}^t(\frac{1}{2}\|x_i-D\alpha_i\|_2^2+\lambda\|\alpha_i\|_1)</script>

    <p><strong><em>converge almost surely to the same limit</em></strong>, and thus that $\hat{f_t}$ acts as a surrogate for $f_t$</p>
  </li>
  <li>Since $\hat{f_t}$ is close to $\hat{f_{t-1}}$ for large values of $t$, so are $D_t$ and $D_{t−1}$, use $D_{t−1}$ as <strong><em>warm restart</em></strong> for computing $D_t$ can acceerate this algorithm</li>
</ul>

<p><br /></p>

<h4 id="sparse-coding">3.2 Sparse Coding</h4>

<p>This is a <strong><em>$l_1$ regularized linear least-squares problem :</em></strong></p>

<script type="math/tex; mode=display">l_1(x,D) = min\frac{1}{2}{\|x-D\alpha\|}_2^2+\lambda\|\alpha\|_1 \quad s.t. \quad \alpha \in R^k</script>

<ul>
  <li>
    <p><strong>Traditional method</strong> (<strong><em>Coordinate descent with soft thresholding</em></strong>)</p>

    <ul>
      <li>
        <p>It is efficient when the columns of the dictionary are <strong><em>low correlated</em></strong></p>
      </li>
      <li>
        <p><strong>Problem:</strong> The columns of learned dictionaries are in general <strong><em>highly correlated</em></strong>, which make this method rather slow</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>New method</strong> (<strong><em>LARS-Lasso algorithm</em></strong>)</p>

    <ul>
      <li>
        <p>With an efficient Cholesky-based implementation, it is at least as fast as the one above</p>
      </li>
      <li>
        <p>It provides the solution with a <strong><em>higher accuracy</em></strong> and being more robust since it does not require an arbitrary stopping criterion</p>
      </li>
    </ul>
  </li>
</ul>

<p><img src="/img/9.JPG" alt="Sparse Coding" /></p>

<p><br /></p>

<h4 id="dictionary-update">3.3 Dictionary Update</h4>

<p><img src="/img/10.JPG" alt="Dictionary update" /></p>

<ul>
  <li>
    <p>The procedure does not require to store all the vectors $x_i$ and $\alpha_i$, but only $A_t$ and $B_t$</p>
  </li>
  <li>
    <p>Sequentially updates each column of $D$</p>
  </li>
  <li>
    <p>Orthogonal projection on to $C$(the constraint set)</p>
  </li>
  <li>
    <p>In practice, the matrix $A_t$ are often <strong><em>concentrated on the diagonal</em></strong>, which makes the block-coordinate descent more efficient</p>
  </li>
</ul>

<p><br /></p>

<h4 id="optimizing-the-algorithm">3.4 Optimizing the Algorithm</h4>

<ul>
  <li>
    <p>3.4.1 Handling fixed-size data sets</p>

    <ul>
      <li>
        <p><strong><em>Problem:</em></strong> When the data sets are predefined and have finite size, the same data points may be examined several times</p>
      </li>
      <li>
        <p><strong><em>Solution:</em></strong> When the training set is small enough, it is possible to further speed up convergence:</p>

        <p>If the sample $x$ has been drawn from the data sets twice in the iteration $t_0$ and $t$, we will replace $\alpha_{t_0}$ by $\alpha_t$ in $A_t$ and $B_t$, that is</p>

        <script type="math/tex; mode=display">A_t = A_{t-1} + \alpha_t\alpha_t^T - \alpha_{t_0}\alpha_{t_0}^T</script>

        <script type="math/tex; mode=display">B_t = B_{t-1} + x_t\alpha_t^T - x_t\alpha_{t_0}^T</script>

        <p>In this setting, we need to store $\alpha_i$ in every iteration and it’s <strong><em>impractical</em></strong></p>

        <p>However, we can sovle this by removing the information from $A_t$ and $B_t$ that is older than two epochs(cycles through the data)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>3.4.2 Scaling te past data</p>

    <ul>
      <li>
        <p><strong><em>Problem:</em></strong> At each iteration, the “new” information $\alpha_t$ that is added to $A_t$ and $B_t$ has the same weight as the “old” one</p>
      </li>
      <li>
        <p><strong><em>Solution:</em></strong> Rescaling the “old” information so that newer coefficients $\alpha_t$ have more weight, which is classical in online learning</p>

        <p>We propose to replace lines 5 and 6 of Algorithm 1 by:</p>

        <script type="math/tex; mode=display">A_t = \beta_t A_{t-1} + \alpha_t\alpha_t^T</script>

        <script type="math/tex; mode=display">B_t = \beta_t B_{t-1} + x_t\alpha_t^T</script>

        <script type="math/tex; mode=display">\beta_t = (1-\frac{1}{t})^\rho</script>

        <p>In this circumstance, we propose</p>

        <script type="math/tex; mode=display">D_t = argmin\frac{1}{\sum_{j=1}^t(j/t)} \sum_{i=1}^t(\frac{i}{t})^\rho(\frac{1}{2}\|x_i-D\alpha_i\|_2^2+\lambda\|\alpha_i\|_1)</script>

        <p>=</p>

        <script type="math/tex; mode=display">argmin\frac{1}{\sum_{j=1}^t(j/t)}(\frac{1}{2}Tr(D^TDA_t)-Tr(D^TB_t))</script>

        <p>When $\rho = 0$, we obtain the original version of the algorithm</p>

        <p><strong><em>In practice, this parameter $\rho$ is only useful for large data sets $(n \ge 100000)$</em></strong></p>
      </li>
    </ul>
  </li>
</ul>


                <hr>

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2016/09/17/Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding/" data-toggle="tooltip" data-placement="top" title="Online Detection of Unusual Events in Videos via Dynamic Sparse Coding">&larr; Previous Post</a>
                    </li>
                    
                    
                </ul>

            </div>
        </div>
    </div>
</article>

<hr>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="https://github.com/icbcbicc">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a href="mailto:icbcbicc@hotmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">Copyright &copy; Jam's Blog 2016</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>

    


</body>

</html>
