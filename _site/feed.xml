<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="http://icbcbicc.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://icbcbicc.github.io/" rel="alternate" type="text/html" /><updated>2016-12-12T14:45:55+08:00</updated><id>http://icbcbicc.github.io/</id><title>Jam&#39;s Blog</title><subtitle>Write your site description here. It will be used as your sites meta description as well!</subtitle><entry><title>中画幅相机入坑记</title><link href="http://icbcbicc.github.io/2016/12/12/Medium-Format-Camera/" rel="alternate" type="text/html" title="中画幅相机入坑记" /><published>2016-12-12T00:00:00+08:00</published><updated>2016-12-12T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/12/12/Medium Format Camera</id><content type="html" xml:base="http://icbcbicc.github.io/2016/12/12/Medium-Format-Camera/">&lt;h2 id=&quot;section&quot;&gt;中画幅相机入坑记&lt;/h2&gt;

&lt;p&gt;============== 坐等收货中 ==============&lt;/p&gt;

&lt;p&gt;国内淘宝咸鱼如果要买同等成色的价格至少2500+RMB。。
然而ebay上的价格140美元，日本到中国运费40美元，虽然运费贵但加起来也就1200RMB。&lt;/p&gt;

&lt;p&gt;运费这么贵的主要原因是相机将近2公斤。为啥我买的相机都这么重呢（135我用的minolta SRT 101，俗称坦克）？可能是不太喜欢塑料件吧。&lt;/p&gt;

&lt;p&gt;以下是卖家描述以及图片。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/17.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/18.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/19.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/20.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/21.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/22.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/23.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/24.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/25.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[Exc+++] Mamiya C33 Professional TLR Camera w/ 80mm f/2.8 Lens from Japan 5226&lt;/p&gt;

&lt;p&gt;Description&lt;br /&gt;
Grade:&lt;br /&gt;
Excellent +++ Condition&lt;/p&gt;

&lt;p&gt;Appearance of the item&lt;br /&gt;
Very Beautiful.&lt;br /&gt;
Malt degradation （查了半天发现malt plane指的是胶片后面那块板子，这和malt到底有啥关系？）&lt;/p&gt;

&lt;p&gt;It works properly.&lt;br /&gt;
Shutter Speed Slightly sticky （貌似这是古董镜间快门相机的通病，主要原因是缺少润滑油）&lt;/p&gt;

&lt;p&gt;Optical  system
There is No marking.&lt;br /&gt;
There is No fungus.&lt;br /&gt;
There is No scratch.&lt;br /&gt;
There is No haze.&lt;br /&gt;
There is No balsam separation.&lt;br /&gt;
Almost no dusts.&lt;br /&gt;
简直完美，这种相机快门和光圈都在镜头内，所以只要镜头问题不大就好。  而且听说这只80mm，2.8的头在双反界名气也不错。
mamiya的C系列双反镜头主要分为前期的chrome和后期的sekor。sekor又分为 早期镀铬 和 中期蓝点 和 后期蓝点S 。
貌似越往后镜头成像和操控越好，但是塑料部件越多。前后期镜头区别主要在于镀膜，早期的镀膜抗眩光能力较差。
但是我看了一下别人用chrome拍的照片，也很漂亮，而且还有旋焦效果。所以我想这几支镜头应该是区别不大吧（自我安慰）。&lt;/p&gt;

&lt;p&gt;Includings&lt;br /&gt;
Mamiya C33 Professional&lt;br /&gt;
Mamiya Sekor C 80mm f/2.8 Lens  （66画幅的80mm等效于135画幅的50mm，我也用的比较习惯）&lt;br /&gt;
Lens cap&lt;br /&gt;
Release（镜间快门加快门线，稳！如果反光板能够预升就更好了。）&lt;/p&gt;</content><author><name>icbcbicc</name></author><category term="photography" /><category term="camera" /><summary>中画幅相机入坑记</summary></entry><entry><title>State-of-the-art in Visual Attention Modeling</title><link href="http://icbcbicc.github.io/2016/12/06/State-of-the-art-in-Visual-Attention-Modeling/" rel="alternate" type="text/html" title="State-of-the-art in Visual Attention Modeling" /><published>2016-12-06T00:00:00+08:00</published><updated>2016-12-06T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/12/06/State-of-the-art in Visual Attention Modeling</id><content type="html" xml:base="http://icbcbicc.github.io/2016/12/06/State-of-the-art-in-Visual-Attention-Modeling/">&lt;h2 id=&quot;state-of-the-art-in-visual-attention-modeling&quot;&gt;State-of-the-art in Visual Attention Modeling&lt;/h2&gt;

&lt;p&gt;Authors: Borji, Ali, Itti, Laurent&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/document/6180177/?arnumber=6180177&amp;amp;tag=1&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;1. INTRODUCTION&lt;/h3&gt;

&lt;h4 id=&quot;definition&quot;&gt;1.1 Definition&lt;/h4&gt;

&lt;h4 id=&quot;origins&quot;&gt;1.2 Origins&lt;/h4&gt;

&lt;h4 id=&quot;empirical-foundations&quot;&gt;1.3 Empirical Foundations&lt;/h4&gt;

&lt;h4 id=&quot;applications&quot;&gt;1.4 Applications&lt;/h4&gt;

&lt;h3 id=&quot;categorization-factors&quot;&gt;2. CATEGORIZATION FACTORS&lt;/h3&gt;

&lt;h4 id=&quot;bottom-upf1-vs-top-down-modelsf2&quot;&gt;2.1 Bottom-up（f1） vs. Top-down Models（f2）&lt;/h4&gt;

&lt;p&gt;模型一般分为这2类，一种流行的说法是这两者共同作用引导了我们的注意力。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;bottom-up一般是基于场景的特征，也就是（stimulus-driven）。&lt;/p&gt;

    &lt;p&gt;能吸引注意力的特征一般都是能够和周围环境明显区分开的。这种模型速度快，一般是feed-forward的，不需feedback。然而这种模型只能解释一部分眼动的现象，因为很多的眼动都是task-driven的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;top-down一般是基于认知的，比如知识、预期、目标、回报等，也就是（goal-driven）的。&lt;/p&gt;

    &lt;p&gt;这种模型速度慢，需要feedback和训练，一般针对某种特定的问题。给定不同的任务去观察同一幅图像，眼动数据差别很大。&lt;/p&gt;

    &lt;p&gt;那么在top-down模型下，我们怎么决定眼动呢？一般有一下3种特征我们需要关注。&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Object Features&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;我们想要寻找的目标的特征，这就是target-driven attention guidance。Guided search theory 指出对不同的目标的attention是有区别的，这可以通过调整不同特征对attention的贡献的权重来实现。&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Scene context&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;人通过很短时间（少于80ms）观察一幅图像可以获得一些很模糊的基本信息（gist）。尽管不能观察到很多细节，但这些信息可用于比较粗糙的场景区分（不是语义（semantic）上的分类）。这种方式对于大规模的场景搜索很有用。&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Task demands&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;任务和眼动的关系非常紧密。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;spatial-vs-spatio-temporal-models-f3&quot;&gt;2.2 Spatial vs. Spatio-temporal Models （f3）&lt;/h4&gt;

&lt;h4 id=&quot;overt-vs-covert-attention&quot;&gt;2.3 Overt vs. Covert attention&lt;/h4&gt;

&lt;h4 id=&quot;space-based-vs-object-based-models-f9&quot;&gt;2.4 Space-based vs. Object-based Models （f9）&lt;/h4&gt;

&lt;h4 id=&quot;features&quot;&gt;2.5 Features&lt;/h4&gt;

&lt;p&gt;显著性的计算模型主要使用了3种特征：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;intensity（intensity contrast，luminance contrast）&lt;/p&gt;

    &lt;p&gt;通常用3个颜色通道的平均值表示，并使用center-surround处理。这种处理模仿了lateral geniculate nucleus（LGN）和V1 cortex中神经元的反馈。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;color&lt;/p&gt;

    &lt;p&gt;用红-绿通道、蓝-黄通道实现，或者用其他色彩空间也行，比如HSV或者Lab。这模仿了V1 cortex中color-opponent neurons的行为。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;orientation&lt;/p&gt;

    &lt;p&gt;通常用oriented Gabor filters实现的卷积来实现，或者用oriented masks来实现。这个特征的灵感源于从MT和MST区域中的对方向和动作有选择作用的神经元。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stimuli-and-task-type&quot;&gt;2.6 Stimuli and Task Type&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;刺激有2种分类方式：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;静态的和动态的&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;人造的和自然的&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;任务通常可以分为3类&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Free viewing tasks&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Visual search tasks&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Interactive tasks&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;evaluation-measures&quot;&gt;2.7 Evaluation Measures&lt;/h4&gt;

&lt;p&gt;将算法输出的saliency map（S）与眼动数据（G）比对。&lt;/p&gt;

&lt;p&gt;可以将评估方式划分为3类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于点&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于区域&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主观评价（好、可接受、坏）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下是几种常见的评估方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;概率分布模型（&lt;strong&gt;Kullback-Leibler (KL) Divergence&lt;/strong&gt;）&lt;/p&gt;

    &lt;p&gt;KL通常用于测量2个概率分布之间的距离。对于saliency评价，一般将预测出的saliency map与随机眼动数据进行比较，KL偏差越大，则表示模型有更好的预测性能。因为人一般关注图像中的很小的局部，这部分有较高的反馈值，而对于大多数区域，反馈值较低。&lt;/p&gt;

    &lt;p&gt;这种评价方式有以下几种好处：&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;KL对概率分布的各种区别都比较敏感。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;KL对reparameterizations有不变性，比如对分布使用连续单调的非线性函数而不影响评价。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;当然KL也有劣势：KL没有一个上界，当2个分布完全不重合时，KL偏差将为无穷大。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2分类模型（&lt;strong&gt;Area Under Curve (AUC)&lt;/strong&gt;）&lt;/p&gt;

    &lt;p&gt;当作2分类问题，针对saliency map中的每个像素，设置不同的threshold画出ROC曲线，用AUC来评估分类器性能。对reparameterizations有不变性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;随机变量模型&lt;/p&gt;

    &lt;p&gt;将S和G当作随机变量&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Normalized Scanpath Saliency (NSS)&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;对reparameterizations有不变性&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Linear Correlation Coefficient (CC)&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;此方法计算2个变量之间的线性关系的强度，常用与评估2张图像之间的关联。当CC值为+1或-1时，2个变量有完美的线性关系。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;String Editing Distance&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;datasets&quot;&gt;2.8 Datasets&lt;/h4&gt;

&lt;p&gt;眼动数据包含了图像（用于研究静止注意力）和视频（用于研究动态注意力）。有时候可以用鼠标的追踪来预测眼动，因为这2者有一定的关联，尽管鼠标的追踪有较多的噪声。&lt;/p&gt;

&lt;h3 id=&quot;attention-models&quot;&gt;3. ATTENTION MODELS&lt;/h3&gt;

&lt;p&gt;我们的重点在于saliency的模型，也就是解释attention behavior。而不是saliency detection，也就是检测分割最显著的区域，尽管这类问题在最初的阶段也使用了saliency operator。&lt;/p&gt;

&lt;p&gt;以下模型按事件先后顺序列出。&lt;/p&gt;

&lt;h4 id=&quot;cognitive-models&quot;&gt;3.1 Cognitive Models&lt;/h4&gt;

&lt;p&gt;Cognitive模型是基于生物原理的，，几乎所有的模型都直接或者间接地受到了它的影响。最初的模型（Itti et al.）使用了颜色、强度、方向作为3个基本的特征通道。这个模型后来也成为了benchmark。此模型主要由以下几个步骤组成：&lt;/p&gt;

&lt;p&gt;将原图像下采样为Gaussian pyramid，其中每一层都由Red(&lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;)\Green(&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;),Blue(&lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;)\Yellow(&lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;),Intensity(&lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;),local orientations($$O_\theta) 这些通道组成。
对patch的每一个通道计算特征并normalize，得出feature map。
将patch的feature map按照不同的feature分别相加，normalize，得出conspicuity maps。
将不同feature的conspicuity maps相加，得到saliency map。&lt;/p&gt;

&lt;h4 id=&quot;bayesian-models&quot;&gt;3.2 Bayesian Models&lt;/h4&gt;

&lt;p&gt;基本原理是基于贝叶斯概率模型，将sensory evidence和prior knowledge结合。&lt;/p&gt;

&lt;h4 id=&quot;decision-theoretic-models&quot;&gt;3.3 Decision Theoretic Models&lt;/h4&gt;

&lt;h4 id=&quot;information-theoretic-models&quot;&gt;3.4 Information Theoretic Models&lt;/h4&gt;

&lt;p&gt;有点像检测不常见的特征。&lt;/p&gt;

&lt;h4 id=&quot;graphical-models&quot;&gt;3.5 Graphical Models&lt;/h4&gt;

&lt;p&gt;概率图模型&lt;/p&gt;

&lt;h4 id=&quot;spectral-analysis-models&quot;&gt;3.6 Spectral Analysis Models&lt;/h4&gt;

&lt;p&gt;频率分析模型。暂时缺乏生物学上的解释。&lt;/p&gt;

&lt;h4 id=&quot;pattern-classification-models&quot;&gt;3.7 Pattern Classification Models&lt;/h4&gt;

&lt;p&gt;使用机器学习。&lt;/p&gt;

&lt;h4 id=&quot;other-models&quot;&gt;3.8 Other Models&lt;/h4&gt;

&lt;h3 id=&quot;discussion&quot;&gt;4. Discussion&lt;/h3&gt;

&lt;p&gt;此部分就现有模型都应该关注的几个问题进行了讨论。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;1. 判断模型是否有生物学理论的支撑&lt;/h4&gt;

&lt;p&gt;历史上还没有能够直接判断的依据，不过一般来说有生物学理论支撑的模型都有更好的效果，比如Decision theoretic 和 AWS model。因此建立一个用于判断 模型在生物学上的可解释性、合理性 的标准是很重要的。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;2. 模型的评价标准&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;不同模型对图像边缘的不同的处理会对最终结果造成很大的影响，因此应该尽可能消除这种影响。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;一般来说人的注意力都是偏向中心的（很多数据集也是），因此一些有中心偏向（center-bias）的模型一定会比其他模型效果好，比如说trivail Gaussian blob model。因此消除这种不平等的因素也很重要。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;covert-attention&quot;&gt;3. covert attention研究的缺失。&lt;/h4&gt;

&lt;p&gt;目前的数据集只能显示出overt attention，比如说眼动数据，然而人眼没有特意去关注的物体也有可能被人covertly施加了关注。&lt;/p&gt;

&lt;h4 id=&quot;interactive--multimodal-&quot;&gt;4. 在interactive 环境下 multimodal 数据集的缺失&lt;/h4&gt;

&lt;h4 id=&quot;top-down&quot;&gt;5. top-down模型的缺失&lt;/h4&gt;

&lt;p&gt;尽管top-down的特征对于注意力很重要，但现在大部分的模型都是bottom-up的。前向（feed-forward）bottom-up模型一般不需要训练，比较简单。而top-down模型普遍需要回馈（feedback），也需要进行训练来适应某种特定的任务，比较复杂。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;6. 不同模型需要的参数数量差别很大&lt;/h4&gt;

&lt;p&gt;基于Gabor 或者 DOG filters的模型需要大量参数。&lt;/p&gt;

&lt;p&gt;spectral saliency 模型需要很少的参数。&lt;/p&gt;

&lt;h3 id=&quot;summary-and-conclusion&quot;&gt;5. SUMMARY AND CONCLUSION&lt;/h3&gt;</content><author><name>icbcbicc</name></author><category term="saliency" /><summary>State-of-the-art in Visual Attention Modeling</summary></entry><entry><title>Matlab数值计算</title><link href="http://icbcbicc.github.io/2016/11/13/Matlab_Numerical_Methods/" rel="alternate" type="text/html" title="Matlab数值计算" /><published>2016-11-13T00:00:00+08:00</published><updated>2016-11-13T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/11/13/Matlab_Numerical_Methods</id><content type="html" xml:base="http://icbcbicc.github.io/2016/11/13/Matlab_Numerical_Methods/">&lt;h2 id=&quot;matlab&quot;&gt;Matlab数值计算&lt;/h2&gt;

&lt;h3 id=&quot;matlab-1&quot;&gt;1. Matlab常用函数补充&lt;/h3&gt;

&lt;p&gt;其他&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt; ... &lt;/code&gt; 表示一行未结束，换行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据结构&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cell&lt;/code&gt;创建元胞，访问元胞用&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt;创建结构体，它是有名字的元胞。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;矩阵&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;A(end)&lt;/code&gt; 可以访问数组最后的一个元素。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rand&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;randn&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;randi&lt;/code&gt; 产生的随机数分别为均匀随机分布、正态随机分布、均匀随机分布整数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;diag&lt;/code&gt; 对角矩阵，&lt;code class=&quot;highlighter-rouge&quot;&gt;blkdiag&lt;/code&gt; 对角块矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sqrtm&lt;/code&gt; 矩阵的主平方根。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hilb&lt;/code&gt;希尔伯特矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hadamard&lt;/code&gt;阿达马(Hadamard)矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wilkinson&lt;/code&gt;威尔金森(Wilkinson)矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gallery&lt;/code&gt;可用于生成多种特殊的矩阵。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;字符串&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;findstr&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;strrep&lt;/code&gt; 分别为字符串种查找字串、替换字串。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2维绘图&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fplot&lt;/code&gt; 画图时自动确定采样频率。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;y=@(t) sin(x.^3)&lt;/code&gt; 定义了一个匿名函数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;xlim&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;ylim&lt;/code&gt; 限制了坐标轴的显示范围，常用于看图像的局部。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3维绘图&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;meshgrid&lt;/code&gt; 生成一个2维的完全点集。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;surfl&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;contour&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;contourf&lt;/code&gt; 分别表示曲面、三维等高线、2维填充等高线。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section&quot;&gt;2. 线性方程组和特征系统&lt;/h3&gt;

&lt;p&gt;线性方程组$Ax=b$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;可用&lt;code class=&quot;highlighter-rouge&quot;&gt;x = A\b&lt;/code&gt;求解，相当于&lt;code class=&quot;highlighter-rouge&quot;&gt;x=b/A&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;x=inv(A)*b&lt;/code&gt;。&lt;code class=&quot;highlighter-rouge&quot;&gt;\&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt;运算符会根据矩阵的特征采用不同的算法，可以求欠定、正定、超定等等各种方程。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;利用&lt;code class=&quot;highlighter-rouge&quot;&gt;inv(A) = adj(A) / |A|&lt;/code&gt;来求逆的效率很低，尽量少用。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;b=0&lt;/code&gt; 则为 &lt;strong&gt;齐次&lt;/strong&gt; 方程组。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;|A| = 0&lt;/code&gt; 表示矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;是 &lt;strong&gt;奇异&lt;/strong&gt; 的，他的逆矩阵不存在，此时方程组无解或者有无穷多解。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rank(A)&lt;/code&gt;表示矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;的 &lt;strong&gt;秩&lt;/strong&gt;，它代表了矩阵中线性无关的行(或列)的个数，同时也是 &lt;strong&gt;约化阶梯矩阵&lt;/strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rref(A)&lt;/code&gt;非零行的个数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当&lt;code class=&quot;highlighter-rouge&quot;&gt;size(A) = [m,n]&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;rank(A) = min(m,n)&lt;/code&gt;表示&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;&lt;strong&gt;满秩&lt;/strong&gt;，也就是方程有唯一解。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;超定&lt;/strong&gt;：方程个数大于未知数个数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;病态方程组&lt;/strong&gt; 是指&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;中元素微小的变化都会导致解的巨大变化，在近似解时会很大的影响精度。
可用&lt;code class=&quot;highlighter-rouge&quot;&gt;cond&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;rcond&lt;/code&gt;来判断矩阵的良态。
前者的范围是 1 到正无穷，1 表示完美良态。
后者从 0 到 1 ，0 表示完美良态。&lt;code class=&quot;highlighter-rouge&quot;&gt;rcond&lt;/code&gt;的计算比较不准确，但速度快。
希尔伯特矩阵是非常病态的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;不是方阵，无法求逆。可以使用 &lt;strong&gt;伪逆&lt;/strong&gt;$A^+$&lt;br /&gt;
$A^+=(A^TA)^{-1}A^T \quad m&amp;gt;n$&lt;br /&gt;
$A^+=A^T(AA^T)^{-1} \quad m&amp;lt;n$，此公式要求A是满秩的。&lt;br /&gt;
当然不满秩也能求，$A^+=VS^TSV^T \quad s.t. \quad [U,S,V]= svd(A)$。&lt;br /&gt;
使用&lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt;即可自动求伪逆而不用关心是否满秩和m、n的大小。&lt;br /&gt;
伪逆常用于求解超定或者欠定方程组，但其实&lt;code class=&quot;highlighter-rouge&quot;&gt;\&lt;/code&gt;符号已经包括了这个功能。&lt;br /&gt;
要求非负的解可用&lt;code class=&quot;highlighter-rouge&quot;&gt;lsqnonneg&lt;/code&gt;，这是一个非负的最小二乘问题。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;稀疏矩阵&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sparse(A)&lt;/code&gt;将&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;转化为稀疏形式，当然前提是&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;本身是稀疏的。&lt;code class=&quot;highlighter-rouge&quot;&gt;full&lt;/code&gt;是逆过程。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;issparse&lt;/code&gt;可判断函数是否稀疏。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nnz&lt;/code&gt;给出矩阵中非0元素个数。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sprandn&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;sprandsys&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;sprandn&lt;/code&gt;分别生成稀疏正态分布随机矩阵、稀疏随机对称矩阵、稀疏正态分布随机整数矩阵。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;利用稀疏矩阵能极大加速运算。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;eig&lt;/code&gt;用于求特征值。&lt;code class=&quot;highlighter-rouge&quot;&gt;eigs&lt;/code&gt;求部分特征值。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;3. 非线性方程组的解&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;二分法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;不动点（迭代）法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;牛顿法 &lt;code class=&quot;highlighter-rouge&quot;&gt;fnewton&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;贝尔斯托法 &lt;code class=&quot;highlighter-rouge&quot;&gt;bairstow&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;roots&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;施罗德法 &lt;code class=&quot;highlighter-rouge&quot;&gt;schroder&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;布罗伊登法 &lt;code class=&quot;highlighter-rouge&quot;&gt;broyden&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fzero&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TODO：各种方法的适用场景&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;4. 函数拟合&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>icbcbicc</name></author><category term="matlab" /><summary>Matlab数值计算</summary></entry><entry><title>The DCP Ruleset</title><link href="http://icbcbicc.github.io/2016/11/02/DCP_ruleset/" rel="alternate" type="text/html" title="The DCP Ruleset" /><published>2016-11-02T00:00:00+08:00</published><updated>2016-11-02T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/11/02/DCP_ruleset</id><content type="html" xml:base="http://icbcbicc.github.io/2016/11/02/DCP_ruleset/">&lt;h2 id=&quot;the-dcp-ruleset&quot;&gt;The DCP Ruleset&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;1. 函数的分类&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;常量&lt;/li&gt;
  &lt;li&gt;仿射&lt;/li&gt;
  &lt;li&gt;凸&lt;/li&gt;
  &lt;li&gt;凹&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;定义：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/13.JPG&quot; alt=&quot;A taxonomy of curvature&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这几个分类是有&lt;strong&gt;重叠&lt;/strong&gt;的：比如常量属于仿射，仿射既属于凸也属于凹。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2. 最主要的规则&lt;/h3&gt;

&lt;p&gt;CVX支持以下3种DCP&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;最小化问题
    &lt;ol&gt;
      &lt;li&gt;目标函数为凸&lt;/li&gt;
      &lt;li&gt;约束条件可以有任意个&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最大化问题
 	1. 目标函数为凹
 	2. 约束条件可以有任意个&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;可行性问题
 	1. 没有目标函数
 	2. 有1个或以上的约束条件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;3. 约束条件&lt;/h3&gt;

&lt;p&gt;CVX支持以下3种约束条件&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$=$ : 两侧都必须是仿射&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\preceq$：左侧为凸，右侧为凹&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\succeq$：左侧为凹，右侧为凸&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;3.1 具体要求&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;不支持&lt;code class=&quot;highlighter-rouge&quot;&gt;!=&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;~=&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;复数
    &lt;ul&gt;
      &lt;li&gt;等式约束的两侧可以是复数，可以分解为2个实数的约束。&lt;/li&gt;
      &lt;li&gt;不等式约束的两侧不能有复数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;集合元素：集合元素的约束必须是等式，且等式两侧必须为仿射&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;严格不等（strict inequalities）
    &lt;ul&gt;
      &lt;li&gt;$\prec$&lt;/li&gt;
      &lt;li&gt;$\succ$&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;因为一些数学原理和凸优化算法的原因，CVX不能保证不等式被严格遵守，因此应该尽量不使用严格不等。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;如果模型中必须用到严格不等，可以使用以下方法 &lt;em&gt;消除严格不等&lt;/em&gt; :&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;采用 &lt;strong&gt;正规化&lt;/strong&gt; 的方法来使其符合CVX的要求。&lt;/p&gt;

    &lt;p&gt;例如：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;A x = 0, \quad C x \preceq 0, \quad x \succ 0&lt;/script&gt;

    &lt;p&gt;可以转化为&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;A x = 0, \quad C x \preceq 0, \quad x \succeq 0, \quad \mathbf{1}^T x = 1&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;添加一个 &lt;strong&gt;偏差(offset)&lt;/strong&gt; 将原式变为非严格不等。&lt;/p&gt;

    &lt;p&gt;例如:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;x &gt; 0 转化为 x \ge 1e-4&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;4. 表达式规则&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;常量表达式： 可以产生有限结果的表达式&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;仿射表达式：
    &lt;ul&gt;
      &lt;li&gt;常量表达式&lt;/li&gt;
      &lt;li&gt;已声明的变量&lt;/li&gt;
      &lt;li&gt;可产生有限结果的函数调用&lt;/li&gt;
      &lt;li&gt;仿射表达式的加减组合&lt;/li&gt;
      &lt;li&gt;常量和仿射表达式的乘积&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;凸表达式
    &lt;ul&gt;
      &lt;li&gt;常量或者仿射表达式&lt;/li&gt;
      &lt;li&gt;可产生凸结果的函数调用&lt;/li&gt;
      &lt;li&gt;仿射标量的偶数次方（不包括0次方）&lt;/li&gt;
      &lt;li&gt;2次凸标量&lt;/li&gt;
      &lt;li&gt;多个凸表达式的和&lt;/li&gt;
      &lt;li&gt;凸表达式和凹表达式的差&lt;/li&gt;
      &lt;li&gt;非负常量和凸表达式的乘积&lt;/li&gt;
      &lt;li&gt;非正常量和凹表达式的乘积&lt;/li&gt;
      &lt;li&gt;凹表达式取反&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;凹表达式
    &lt;ul&gt;
      &lt;li&gt;常量或者仿射表达式&lt;/li&gt;
      &lt;li&gt;可产生凹结果的函数调用&lt;/li&gt;
      &lt;li&gt;凹标量的$p$次方($0&amp;lt; p &amp;lt; 1$)&lt;/li&gt;
      &lt;li&gt;2次凹标量&lt;/li&gt;
      &lt;li&gt;多个凹表达式的和&lt;/li&gt;
      &lt;li&gt;凹表达式和凸表达式的差&lt;/li&gt;
      &lt;li&gt;非负常量和凹表达式的乘积&lt;/li&gt;
      &lt;li&gt;非正常量和凸表达式的乘积&lt;/li&gt;
      &lt;li&gt;凸表达式取反&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;任何不符合以上规则的表达式都会被CVX禁止，尽管它可能是凸的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;涉及到矩阵时以其中的元素作为考量。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;不支持非标量之间的乘法&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;例如： &lt;code class=&quot;highlighter-rouge&quot;&gt;x*sqrt{x}&lt;/code&gt; 不被接受，但可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;pow_p(x, 3/2)&lt;/code&gt; 替代。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;5. 函数&lt;/h3&gt;

&lt;p&gt;CVX中的函数有2个特征：曲度(curvature)和单调性。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;变量必须在函数的隐含定义域中。只需定义用户自定的定义域。
  例如，不用为添加$x\ge0$的约束。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CVX判断凹凸时不考虑隐含的定义域或者自定的定义域，在定义域外为$+\inf$的被认为是凸，反之为凹。&lt;/p&gt;

    &lt;p&gt;例如：$\frac{1}{x} \quad s.t. x \ge 1$ 不被CVX认为是凸的，尽管它事实上是。解决方法是将$x &amp;lt; 0$ 的区域的函数值定义为$+\inf$，可以用CVX的函数 &lt;code class=&quot;highlighter-rouge&quot;&gt;inv_pos(x)&lt;/code&gt; 来完成。反之， &lt;code class=&quot;highlighter-rouge&quot;&gt;-inv_pos(-x)&lt;/code&gt; 将表示凹的版本。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CVX判断单调性时不考虑定义域，比如$\sqrt{x}$在$x &amp;lt; 0$的区域任然被认为是非减的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多元函数的凹凸性是由所有变量联合决定的，但单调性可以按每个变量来讨论。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;有些多元函数仅对一部分参数是凸的，凹的，仿射的，此时其他参数必须设为常数。比如：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;norm(x, p) \quad s.t. \quad p \ge 1&lt;/script&gt;

    &lt;p&gt;仅仅对$x$是凸的，因此在CVX中$p$必须设为常数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;函数嵌套：&lt;/p&gt;

    &lt;p&gt;凸函数、凹函数、仿射函数可以接受一个仿射表达式，结果相应为凸、凹、仿射的。&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;当已知 &lt;em&gt;函数为凸&lt;/em&gt; 和函数对于各个参数的单调性时：&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;函数对于某参数不减：这个参数必须是凸的&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;函数对于某参数不增：这个参数必须是凹的&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;函数对于某参数不减不增：这个参数必须是仿射的&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;当每个参数都满足以上条件，这个函数就是凸的并且可以被CVX接受。函数为凹时第1，2条相反，第3条相同。&lt;/p&gt;

    &lt;p&gt;例如：&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;max(abs(x))&lt;/code&gt;，其中&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;时向量。&lt;code class=&quot;highlighter-rouge&quot;&gt;max&lt;/code&gt;函数是凸的并且对于任一参数不减，并且&lt;code class=&quot;highlighter-rouge&quot;&gt;abs&lt;/code&gt;函数也是凸的，因此整体是凸的。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum(sqrt(x))&lt;/code&gt;，因为&lt;code class=&quot;highlighter-rouge&quot;&gt;sum&lt;/code&gt;函数是仿射且不减的，而&lt;code class=&quot;highlighter-rouge&quot;&gt;sqrt&lt;/code&gt;是凹的，因此整体为凹。同理&lt;code class=&quot;highlighter-rouge&quot;&gt;sum(square(x))&lt;/code&gt;是凸的。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;非线性嵌套的单调性&lt;/p&gt;

    &lt;p&gt;因为很多凸函数在定义域上没有单调性，因此不能直接使用（自行定义定义域也不行）。CVX的很多函数有2种形式，一种是原始的，一种是在固定定义域有单调性的。例如：
  &lt;code class=&quot;highlighter-rouge&quot;&gt;square_pos()&lt;/code&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_pos()&lt;/code&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;quad_pos_over_lin()&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;关于平方&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;事实上，CVX原本并不支持平方表达式，但CVX将自动识别以下几种边的表达式&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$x . * x$&lt;/li&gt;
  &lt;li&gt;$conj( x ) . * x$&lt;/li&gt;
  &lt;li&gt;$y’ * y$&lt;/li&gt;
  &lt;li&gt;$(A * x - b)’ * Q * (Ax-b)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;并将其转为相应的可接受的形式：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_abs( y )&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_abs( y )&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_abs( y )&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;quad_form( A * x - b, Q )&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CVX建议尽量少用平方，因为平方是平滑的，它一般是作为非平滑的真实目标函数的替代，然而CVX本身支持很多的非平滑函数。因此使用非平方可以更加精确。例如： &lt;code class=&quot;highlighter-rouge&quot;&gt;sum( ( A * x - b ) . ^ 2 ) &amp;lt;= 1&lt;/code&gt; 可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;norm( A * x - b )&amp;lt;= 1&lt;/code&gt; 替代，也就是欧式范数。&lt;/p&gt;</content><author><name>icbcbicc</name></author><category term="convex optimization" /><summary>The DCP Ruleset</summary></entry><entry><title>Sparse Reconstruction Cost for Abnormal Event Detection</title><link href="http://icbcbicc.github.io/2016/10/23/Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection/" rel="alternate" type="text/html" title="Sparse Reconstruction Cost for Abnormal Event Detection" /><published>2016-10-23T00:00:00+08:00</published><updated>2016-10-23T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/23/Sparse Reconstruction Cost for Abnormal Event Detection</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/23/Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection/">&lt;h1 id=&quot;sparse-reconstruction-cost-for-abnormal-event-detection&quot;&gt;Sparse Reconstruction Cost for Abnormal Event Detection&lt;/h1&gt;
&lt;p&gt;Authors: &lt;em&gt;Yang Cong, Junsong Yuan, Ji Liu&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;http://www.cs.rochester.edu/~jliu/paper/Cong-Yuan-CVPR11.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;conventional-algorithms&quot;&gt;Conventional Algorithms&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Probability Model&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Detect testing sample with lower probability as anormaly by fitting a probability model to the training data. However, the required number of training data increases exponentially with the feature dimension, and it’s unrealistic to collect enough data for density estimation in practice&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sparse-reconstruction-cost-src&quot;&gt;Sparse Reconstruction Cost (SRC)&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;We propose SRC based on the weighted $l_1$ minimization.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Normal event: generate sparse reconstruction coefficients with a &lt;strong&gt;small SRC&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abnormal events: generate a dense representation with a &lt;strong&gt;large SRC&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;types-of-abnormal-events&quot;&gt;2 types of abnormal events:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Local abnormal event (LAE)&lt;/p&gt;

    &lt;p&gt;The local behavior is different from its spatio temporal neighborhoods&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Global abnormal event (GAE)&lt;/p&gt;

    &lt;p&gt;The whole scene is abnormal, even though any individual lcoal behavior can be normal&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;New dictionary selection method&lt;/p&gt;

    &lt;p&gt;Reduce the size of the basis set $\phi$ for an efficient reconstruction&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>icbcbicc</name></author><summary>Sparse Reconstruction Cost for Abnormal Event Detection
Authors: Yang Cong, Junsong Yuan, Ji Liu
PDF</summary></entry><entry><title>Machine Learning (Zhihua Zhou) Notes 03</title><link href="http://icbcbicc.github.io/2016/10/16/machine_learning-_zhihua_zhou_notes_03/" rel="alternate" type="text/html" title="Machine Learning (Zhihua Zhou) Notes 03" /><published>2016-10-16T00:00:00+08:00</published><updated>2016-10-16T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/16/machine_learning _zhihua_zhou_notes_03</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/16/machine_learning-_zhihua_zhou_notes_03/"></content><author><name>icbcbicc</name></author><category term="machine learning" /><summary></summary></entry><entry><title>Machine Learning (Zhihua Zhou) Notes 02</title><link href="http://icbcbicc.github.io/2016/10/15/machine_learning-_zhihua_zhou_notes_02/" rel="alternate" type="text/html" title="Machine Learning (Zhihua Zhou) Notes 02" /><published>2016-10-15T00:00:00+08:00</published><updated>2016-10-15T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/15/machine_learning _zhihua_zhou_notes_02</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/15/machine_learning-_zhihua_zhou_notes_02/">&lt;h2 id=&quot;section&quot;&gt;3. 线性模型&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;3.2 线性回归&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基本概念&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;线性回归：用线性模型拟合目标函数。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;均方误差：线性回归中常用的性能度量，它的几何意义是欧几里得距离(Euclidean distance)。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;最小二乘法(least square method)：基于均方误差最小化来求解模型的方法。几何意义是在目标空间中找到一个超平面，使样本点到该平面的欧式距离之和最小。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多元线性回归(multivariate linear regression)&lt;/p&gt;

    &lt;p&gt;求最优$\hat{\omega}$使目标函数$E_\hat{\omega}$最小:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E_\hat{\omega} = (y-X\hat{\omega})^T(y-X\hat{\omega})&lt;/script&gt;

    &lt;p&gt;对$\hat{\omega}$求导：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}} = 2X^T(X\hat{\omega}-y)&lt;/script&gt;

    &lt;p&gt;当$X^TX$是满秩矩阵(full-rank matrix)或正定矩阵(positive definite matrix)时，可直接求得最优解：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\omega} = (X^TX)^{-1}X^Ty&lt;/script&gt;

    &lt;p&gt;最终模型：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_i) = x_i^T(X^TX)^{-1}X^Ty&lt;/script&gt;

    &lt;p&gt;然而当$X^TX$不是满秩矩阵时，$X$的列数多于行数，也就是特征数比样本数多。此时方程有多个最优解，需要引入正则化(regularization)来选择一个$\hat{\omega}$。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;广义线性模型(generalized linear model)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;y = g^{-1}(\omega^Tx+b)&lt;/script&gt;

    &lt;p&gt;其中，$g(.)$是单调可微函数。称之为联系函数(link function)&lt;/p&gt;

    &lt;p&gt;较长用的联系函数是$ln(.)$，此时称之为对数线性回归(log-linear regression)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对数几率回归(logistic regression)&lt;/p&gt;

    &lt;p&gt;用于分类任务，将输出映射到${0,1}$的集合上。&lt;/p&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y = \frac{1}{1+e^{-(\omega^Tx+b)}}&lt;/script&gt;
  $\to$
  &lt;script type=&quot;math/tex&quot;&gt;ln\frac{y}{1-y} = \omega^Tx+b&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;若将$y$视为取正例的概率，则$1-y$为取反例的概率。&lt;/p&gt;

    &lt;p&gt;$\frac{y}{1-y}$称为&lt;strong&gt;几率(odds)&lt;/strong&gt;，反映了相对可能性。&lt;/p&gt;

    &lt;p&gt;$ln\frac{y}{1-y}$称为&lt;strong&gt;对数几率(logit)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;可通过&lt;strong&gt;极大似然法(maximum likeihood method)&lt;/strong&gt;估计最优解：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\omega,b) = \sum_{i=1}^{m}ln\{p(y_i|x_i;\omega;b)\}&lt;/script&gt;

    &lt;p&gt;$=$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{m}ln\{y_ip(y=1|x_i;\omega;b)+(1-y_i)p(y=0|x_i;\omega;b)\}&lt;/script&gt;

    &lt;p&gt;$=$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{m}(-y_i(\omega;b)^Tx_i+ln(1+e^{(\omega;b)^T}x_i))&lt;/script&gt;

    &lt;p&gt;此式为关于$(\omega;b)$高阶可导连续凸函数。可用&lt;strong&gt;梯度下降、牛顿法&lt;/strong&gt;进行求解。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-discriminant-analysisldafisher&quot;&gt;3.4 线性判别分析(Linear Discriminant Analysis，LDA，亦称Fisher判别分析)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;主要思想：将共有$N$类的样本投影到一个$N-1$维的空间，使同类的投影点接近，不同类的投影点相互远离。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;指标&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$X_i,\mu_i,\Sigma_i$分别为第$i$类的样本点、均值向量、协方差矩阵。$\mu$为所有样本的均值向量。假设共$N$类，第$i$类有$m_i$个样本，共$m$个样本。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;全局散度矩阵：&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_t = S_b+S_w = \sum_{i=1}^m (x_i-\mu)(x_i-\mu)^T&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;类内散度矩阵：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_w = \sum_{i=1}^N S_{w_i} = \sum_{i=1}^N \sum_{x\in X_i}(x-\mu)(x-\mu)^T&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;类间散度矩阵：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_b = S_t-S_w = \sum_{i=1}^N m_i(\mu_i-\mu)(\mu_i-\mu)^T&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;优化：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;可使用$S_t,S_w,S_b$中的任意2个指标进行优化。常用的优化目标是：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;max\frac{tr(W^TS_bW)}{tr(W^TS_wW)} \quad s.t. \quad W \in R^{d \times (N-1)}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;此式可以通过&lt;strong&gt;拉格朗日乘子法&lt;/strong&gt;转化为广义特征值问题：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_bW = \lambda S_wW&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$W$的闭式解：&lt;strong&gt;$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量&lt;/strong&gt;组成的矩阵。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;3.5 多分类学习&lt;/h3&gt;

&lt;p&gt;将多分类任务拆分为多个2分类任务&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;拆分策略&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;一对一(One vs. One, OvO)：共$N(N-1)/2$个分类器&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;一对多(One vs. Rest, OvR)：共$N$个分类器。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多对多(Many vs. Many, MvM)&lt;/p&gt;

    &lt;p&gt;用、纠错输出码(Error Correcting Output Codes, EOOC)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;对数据进行$M$次划分，得到$M$组{训练集，测试集}，从而得到$M$个分类器。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;对每个样本，分别用$M$个分类器分类，这些预测标记组成一个编码序列。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;将预测编码与每个类别自己的编码(One-hot码)比较，距离(海明距离，欧氏距离)最近的为最终预测结果。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;EOOC对与分类器的错误有一定容忍和修正能力。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;3.6 类别不平衡问题&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;再缩放(rescaling)：$\frac{y}{1-y}&amp;gt;\frac{m^+}{m+-}$：预测为正例。其中$m^+,m^-$分别为正例、反例的数目。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;欠采样(undersampling):去除一部分样本使得样本平衡&lt;/p&gt;

    &lt;p&gt;将类别较多的样本划分为几个集合，形成多组{训练集，测试集}分别学习。虽然每组是欠采样，但全局上却没有欠采样，充分利用了数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;过采样(oversampling):增加一部分样本使得样本平衡&lt;/p&gt;

    &lt;p&gt;不能直接对原有样本进行重复采样，否则将会出现严重的过拟合。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;4.决策树&lt;/h2&gt;

&lt;p&gt;TODO：信息增益、增益率、基尼指数、剪枝、连续值处理、多变量决策树（斜划分）&lt;/p&gt;</content><author><name>icbcbicc</name></author><category term="machine learning" /><summary>3. 线性模型</summary></entry><entry><title>Machine Learning (Zhihua Zhou) Notes 01</title><link href="http://icbcbicc.github.io/2016/10/14/machine_learning_zhihua_zhou_notes_01/" rel="alternate" type="text/html" title="Machine Learning (Zhihua Zhou) Notes 01" /><published>2016-10-14T00:00:00+08:00</published><updated>2016-10-14T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/14/machine_learning_zhihua_zhou_notes_01</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/14/machine_learning_zhihua_zhou_notes_01/">&lt;h1 id=&quot;section&quot;&gt;周志华《机器学习》笔记-01&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 模型的评估与选择&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.1 经验误差与过拟合&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;三种误差&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;训练误差(training error)、经验误差(empirical error)&lt;/strong&gt;：训练集上的误差，受到欠拟合与过拟合的影响&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;测试误差(testing error)&lt;/strong&gt;：测试集上的误差，通过常作为泛化误差的近似&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;泛化误差(generalization error)&lt;/strong&gt;：在新样本上的误差，无法直接获得&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;欠拟合与过拟合&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;欠拟合易于克服：增加训练轮数&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;过拟合是机器学习面临的关键障碍，无法彻底避免。因为机器学习面临的问题通常是$NP$难甚至更难。然而学习算法必然要在多项式时间完成，要想彻底避免过拟合，就要通过误差最小化直接获得最优解，也就意味着$P=NP$。只要坚信$P\ne NP$，过拟合就不可避免。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.2 产生测试集的方法&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2.2.1 留出法(hold-out)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;方法：将数据集分为互斥的两个集合。采用分层采样(stratified sampling)来保证2个集合的数据分布一致性（也就是训练集和测试集中相同类型样本所占的比例相同）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缺陷：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;由于单次使用留出法得到的结果不够可靠（即使保证相同类别样本比例相同，仍然有多种方式划分），一般采用多次随机划分得到多组{训练集，测试集}进行训练，误差取平均的方式。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;训练集占的多：模型精确，但评估结果不够稳定准确；测试集占的多，模型精度不足。此问题没有完美解决方案，一般选取$\frac{2}{3}$到$\frac{4}{5}$的样本用于训练。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.2.2 交叉验证法(cross validation)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;方法：
    &lt;ol&gt;
      &lt;li&gt;先将数据划为$k$个大小相同，数据分布相同的互斥子集&lt;/li&gt;
      &lt;li&gt;每次使用$k-1$个子集进行训练，$1$个做测试。一共可获得$k$组{训练集，测试集}的组合。&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;关键：结果的稳定性和真实性很大程度上取决于$k$的选择，因此这个方法被称作“$k$折交叉验证($k$-fold cross validation)”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缺陷：与留出法相似，将数据划分为$k$组有很多方式，产生了不确定因素。一般采用进行$p$次划分取均值的方法，也就是“$p$次$k$折交叉验证”，一共得到$k \times p$组{训练集，测试集}。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;特例：当$k=样本数$，称为“留一法(Leave-One-Out，LOO)”。此方式不受样本划分的影响，评估较准确，但计算量太大。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.2.3 自助法(bootstrapping)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;以自助采样法(bootstrapping sampling，亦称“可重复采样/有放回采样”)为基础，&lt;strong&gt;适合数据集小的情况&lt;/strong&gt; 。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;方法：在数据集$D$中有放回的随机取出一个样本，重复$m$次,得到$D’$作为训练集，$D-D’$作为测试集。$D’$中可能有重复的样本。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当$m$足够大时，一个样本在$m$次选取中不被选中的概率是
&lt;script type=&quot;math/tex&quot;&gt;$\lim_{m \to \inf}(1-\frac{1}{m})^m \to \frac{1}{e} \approx 0.368&lt;/script&gt;$
因此$D$中有$36.8\%$的样本不在$D’$中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缺陷：改变了初始数据的分布。数据量足够时一般不采用此方法。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.2.4 调参与最终模型&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;两类参数：
    &lt;ol&gt;
      &lt;li&gt;超参数：人工设定算法的参数，数目较少&lt;/li&gt;
      &lt;li&gt;模型参数：由算法生成的参数，数量可以很多&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;在模型最优化后，应该使用所有数据（包括测试集）再训练一次，得到最终模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.3 性能度量&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2.3.2 查准率，查全率与$F$度量&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;混淆矩阵(confusion matrix)：&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt; &lt;/th&gt;
          &lt;th&gt; &lt;/th&gt;
          &lt;th&gt;预测结果&lt;/th&gt;
          &lt;th&gt; &lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;正例&lt;/td&gt;
          &lt;td&gt;反例&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;真实情况&lt;/td&gt;
          &lt;td&gt;正例&lt;/td&gt;
          &lt;td&gt;TP&lt;/td&gt;
          &lt;td&gt;FN&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;反例&lt;/td&gt;
          &lt;td&gt;FP&lt;/td&gt;
          &lt;td&gt;TN&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查准率(precision，亦称准确率)：$P = \frac{TP}{TP+FP}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查全率(recall，亦称召回率)：$R = \frac{TP}{TP+FN}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查全率与查准率通常是矛盾的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;P-R曲线
    &lt;ul&gt;
      &lt;li&gt;方法：
        &lt;ol&gt;
          &lt;li&gt;将测试样本按概率从大到小排序&lt;/li&gt;
          &lt;li&gt;将前$n$个样本作为正例，得到查准率与查全率&lt;/li&gt;
          &lt;li&gt;$n = n+1$&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;利用P-R曲线的评估：
  &lt;img src=&quot;/img/13.png&quot; alt=&quot;P-R curve&quot; /&gt;
        &lt;ol&gt;
          &lt;li&gt;曲线下面积越大，则该模型越好。当一个曲线包住了另一个曲线而没有相交，则认为外面的曲线表示的模型更好（比如下图中的蓝色比绿色好）。&lt;/li&gt;
          &lt;li&gt;$P = R$的点称为平衡点(Break-Even Point, BEP)，也就是曲线与对角线的交点，也可作为评估依据：越大越到。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$F_1$与$F_\beta$度量&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;F1 = \frac{2 \times P \times R}{P+R}&lt;/script&gt;
  实际上就是P与R的调和平均。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;F_\beta = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}&lt;/script&gt;
  实际上就是P与R的加权调和平均。$\beta$决定了查全率和查准率谁更重要：$\beta &amp;gt; 1$时，查全率更重要；$0 &amp;lt; \beta &amp;lt; 1$时，查准率更重要。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;宏查准率，宏查全率，宏$F1$；微查准率，微查全率，微$F1$：略&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.3.3 ROC与AUC&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;截断点(cut point)的选择：&lt;/p&gt;

    &lt;p&gt;将样本按概率从大到小排序&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;查准率高：截断点靠前&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;查全率高：截断点靠后&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ROC(Receiver Operating Characteristic，受试者工作特征)
  &lt;img src=&quot;/img/14.png&quot; alt=&quot;ROC curve&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;纵轴：真正率
  &lt;script type=&quot;math/tex&quot;&gt;TPR = \frac{TP}{TP+FN}&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;横轴：假正率
  &lt;script type=&quot;math/tex&quot;&gt;FPR = \frac{FP}{TN+FP}&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;对角线：随机猜测模型&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;评估方式：与P-R曲线相同。曲线下的面积称为AUC（Aera under ROC）&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.3.4 代价敏感错误率与代价曲线&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;之前的评价方式隐式假设了均等代价，即将错误个数作为代价。在非均等代价下，ROC曲线不能反映出总体代价，此时应该使用代价曲线。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;代价曲线(cost surve)&lt;/strong&gt;
  &lt;img src=&quot;/img/15.png&quot; alt=&quot;cost curve&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;横轴：正例概率代价
  &lt;script type=&quot;math/tex&quot;&gt;P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01}+(1-p) \times cost_{10}}&lt;/script&gt;
其中，$p$为正例的概率；$cost_{01}$为假反率代价；$cost_{10}$为假正率代价。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;纵轴：归一化代价
  &lt;script type=&quot;math/tex&quot;&gt;cost_{norm} = \frac{(1-TPR) \times p \times cost_{01}+FPR \times (1-p) \times cost_{10}}{p \times cost_{01}+(1-p) \times cost_{10}}&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;在ROC曲线上取一点(FPR, TPR)，在代价曲线上将左纵轴上的(0,FPR)与右纵轴上的(1，1-TPR)2点连成线段。所有线段围成的部分面积为&lt;strong&gt;期望总体代价&lt;/strong&gt;。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;2.4 比较检验&lt;/h3&gt;
&lt;p&gt;用于度量模型在测试集上的性能有多大概率是泛化性能，暂略&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.4.1 假设检验&lt;/strong&gt;
&lt;strong&gt;2.4.2 价差验证t检验&lt;/strong&gt;
&lt;strong&gt;2.4.3 McNemar检验&lt;/strong&gt;
&lt;strong&gt;2.4.4 Friedman检验与Nemenyi后续检验&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;2.5 偏差与方差&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;基于均方误差的回归任务的泛化误差可分解为偏差、方差、噪声之和&lt;/strong&gt;。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;偏差：标记与真实类别的偏差。表明了在当前任务上任何学习算法所能达到的期望泛化误差的最小值，是任务本身的属性。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;方差：同样大小的训练集变化所导致的性能变化，即数据扰动造成的影响。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;偏差：算法本身的拟合能力。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;偏差-方差窘境(bias-variance dilemma)&lt;/p&gt;

    &lt;p&gt;偏差与方差一般都是冲突的。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;训练刚开始：拟合能力不足，受数据扰动的影响较小。偏差主导了泛化错误率。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;训练充足：拟合能力很强，受数据扰动的影响大。方差主导了泛化错误率。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attention：对于分类任务，由于0/1损失函数的跳变性，偏差-方差分解很困难。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>icbcbicc</name></author><category term="machine learning" /><summary>周志华《机器学习》笔记-01</summary></entry><entry><title>Online Detection of Abnormal Events Using Incremental Coding Length</title><link href="http://icbcbicc.github.io/2016/10/04/Online-Detection-of-Abnormal-Events-Using-Incremental-Coding-Length/" rel="alternate" type="text/html" title="Online Detection of Abnormal Events Using Incremental Coding Length" /><published>2016-10-04T00:00:00+08:00</published><updated>2016-10-04T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/04/Online Detection of Abnormal Events Using Incremental Coding Length</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/04/Online-Detection-of-Abnormal-Events-Using-Incremental-Coding-Length/">&lt;h1 id=&quot;online-detection-of-abnormal-events-using-incremental-coding-length&quot;&gt;Online Detection of Abnormal Events Using Incremental Coding Length&lt;/h1&gt;

&lt;p&gt;Authors: &lt;em&gt;Jatanta K.Dutta, Bonny Banerjee&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/83f6/d84389dcdc25a4a1462044d7b1cbc2e75eac.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;本文使用稀疏编码进行异常事件检测。数据预处理、特征提取、稀疏编码、字典生成都使用传统方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创新之处在于引进了 Incremental Coding Length (ICL)作为稀疏表达的评价，它代表了每个特征的熵的增加量。异常事件可以由特征的rarity来定义（罕见的特征意味着异常）。最终，将所有特征的energy按权重相加就可以判断是否有异常了。文中将每一个特征的energy作为rarity， 它是关于ICL的函数。ICL的计算不需要任何参数，也不需要关于数据的先验假设&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;1. Abstract&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;事件的异常主要是由2个因素同时决定的:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在稀疏表达事件的过程中，每个特征使用的频率&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;稀疏表达中特征的系数&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;2. Introduction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;异常事件的检测面临3个难点：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;无监督的2分类&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;数据量大，无法全部存储&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;数据的分布不恒定&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;3. Related Work&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;异常事件检测通常由以下几个步骤组成（以及其常用方法）：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;数据预处理，获得低级表达&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Histogram of optical flow (HOF)&lt;/li&gt;
      &lt;li&gt;Muti-scale histogram of optical flow (MHOF)&lt;/li&gt;
      &lt;li&gt;Histogram of optical flow orientation (HOFO)&lt;/li&gt;
      &lt;li&gt;Spatio-temporal gradient&lt;/li&gt;
      &lt;li&gt;3D Spatio-temporal foreground mask&lt;/li&gt;
      &lt;li&gt;Binary features&lt;/li&gt;
      &lt;li&gt;Backgroud segregation (Foreground detection)&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;Mixure of optical flow (MPPCA, temporal features)&lt;/li&gt;
      &lt;li&gt;MPPCA+SF&lt;/li&gt;
      &lt;li&gt;Mixtures of dynamic textures (MDT, spatial and temporal features)&lt;/li&gt;
      &lt;li&gt;Chaotic invariant&lt;/li&gt;
      &lt;li&gt;Social force model (SF, spatial features)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对低级表达进行抽象，得到中级表达
    &lt;ul&gt;
      &lt;li&gt;Sparse coding
        &lt;ol&gt;
          &lt;li&gt;Low rank dictoinary (SparseLR)&lt;/li&gt;
          &lt;li&gt;Compact regularization (SparseCR)&lt;/li&gt;
          &lt;li&gt;LR+CR&lt;/li&gt;
          &lt;li&gt;Weighted sparse representation (SparseW)&lt;/li&gt;
          &lt;li&gt;Sparse combination learning (SCL, 150fps matlab)&lt;/li&gt;
          &lt;li&gt;Large scale dictionary selection (LSDS)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;Hiden Markov model (HMM)&lt;/li&gt;
      &lt;li&gt;Markov random field (MRF, Saligrama)&lt;/li&gt;
      &lt;li&gt;Mixture of probabilistic principle component analysis(MPPCA)&lt;/li&gt;
      &lt;li&gt;Dimensionality reduction(PCA, ICA, clustering)&lt;/li&gt;
      &lt;li&gt;Gaussian mixture model&lt;/li&gt;
      &lt;li&gt;Latent Dirichlet allocation&lt;/li&gt;
      &lt;li&gt;Deep learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对这些表达进行评估，检测出异常（本文所关注的重点）
    &lt;ul&gt;
      &lt;li&gt;Sparse rconstruction error (SRC)&lt;/li&gt;
      &lt;li&gt;Incremental coding lenth (ICL, entropy gain)&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;Prediction error&lt;/li&gt;
      &lt;li&gt;Rarity index&lt;/li&gt;
      &lt;li&gt;Information content&lt;/li&gt;
      &lt;li&gt;Density-based scoring&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;按使用场景分类 (TODO)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Crowded scenario
    &lt;ul&gt;
      &lt;li&gt;Binary features based on back ground model&lt;/li&gt;
      &lt;li&gt;3D Spatio-temporal foreground mask fusing Markov Random Field&lt;/li&gt;
      &lt;li&gt;Trajectory-based approaches&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Uncrowded scenario
    &lt;ul&gt;
      &lt;li&gt;Local abnormal event&lt;/li&gt;
      &lt;li&gt;Global abnormal event&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-framework&quot;&gt;4. Proposed Framework&lt;/h2&gt;

&lt;h3 id=&quot;video-representation&quot;&gt;4.1 Video Representation&lt;/h3&gt;

&lt;p&gt;Spatiotemporal interest point detector&lt;/p&gt;

&lt;h3 id=&quot;online-sparse-dictionary-learning&quot;&gt;4.3 Online Sparse Dictionary Learning&lt;/h3&gt;

&lt;p&gt;Batch Orthogonal Matching Pursuit （Batch OMP）&lt;/p&gt;

&lt;h3 id=&quot;abnoraml-event-detection&quot;&gt;4.3 Abnoraml Event Detection&lt;/h3&gt;

&lt;p&gt;假设对某一时刻的输入数据$X(t) = [x_1(t), …, x_n(t)]$，都有稀疏系数 $\Gamma = [\gamma_1, … , \gamma_n] \in R^{k*n}$&lt;/p&gt;

&lt;p&gt;那么在时刻$t$，对于第 $j$ 个特征的 activity ratio 定义为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j(t) = \frac{\sum_{h=1}^n|\Gamma_{j,h}(t)|}{\sum_{i=1}^k\sum_{h=1}^n|\Gamma_{i,h}(t)|}&lt;/script&gt;

&lt;p&gt;$t$ 时刻的 summary activity ratio 将按照以下方式更新（初始值为$1/k$）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(t) = (1-\alpha(t))q(t-1)+\alpha(t)p(t)&lt;/script&gt;

&lt;p&gt;其中，$\alpha(t)$ 是一个时间的函数。当 $\alpha(t) = 1/t$ , $q(t)$ 就是从开始到现在的平均 activity ratio。当$\alpha(t) = 1/t_1$时， （$t_1$ 是一个正的常数）, $q(t)$ 就是前 $t_1$ 个 activity ratio 的平均值。$\alpha(t) = 1/t_1$ 对于数据分布不稳定的情况很有用。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ICL(q_j) = \frac{\partial {H(q)} }{\partial {q_j} } = -H(q) - q_j - log q_j - q_j log q_j&lt;/script&gt;

&lt;p&gt;在计算完 ICL 后，就得到了任意 $t$ 时刻的显著特征集（salient feature set）&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = \{j | ICL(q_j(t)) &gt; 0\}&lt;/script&gt;

&lt;p&gt;$S(t)$ 中的每一个显著特征的energy表示为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j(t) = \frac{ICL(q_j(t))}{\sum_{i \in S(t)}ICL(q_i(t))} \quad s.t. \quad j \in S(t)&lt;/script&gt;

&lt;p&gt;对于不属于$S(t)$的特征：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j(t) = 0 \quad s.t. \quad j \notin S(t)&lt;/script&gt;

&lt;p&gt;$\theta_j(t)$ 表示了用$j$特征来表示输入的罕见程度&lt;/p&gt;

&lt;p&gt;最终，每一个cube的 anomaly score 定义为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g = |\gamma|^T\theta&lt;/script&gt;

&lt;p&gt;将连续几帧的所有 cube 的 anomaly score 经过 gaussian filter 处理作为每一帧的 anomaly map&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experimental-result&quot;&gt;5. Experimental Result&lt;/h2&gt;

&lt;p&gt;略&lt;/p&gt;</content><author><name>icbcbicc</name></author><summary>Online Detection of Abnormal Events Using Incremental Coding Length</summary></entry><entry><title>The Negative —— Part 2</title><link href="http://icbcbicc.github.io/2016/10/02/The-Negative-Part-2/" rel="alternate" type="text/html" title="The Negative —— Part 2" /><published>2016-10-02T00:00:00+08:00</published><updated>2016-10-02T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/02/The Negative Part 2</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/02/The-Negative-Part-2/">&lt;h2 id=&quot;section&quot;&gt;4. 区域选置&lt;/h2&gt;

&lt;p&gt;由于区域选置这一概念是基于其他重要概念之上的，因此我们先介绍一些基本概念。&lt;/p&gt;</content><author><name>icbcbicc</name></author><summary>4. 区域选置</summary></entry></feed>
