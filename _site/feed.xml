<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="http://icbcbicc.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://icbcbicc.github.io/" rel="alternate" type="text/html" /><updated>2016-11-13T12:38:16+08:00</updated><id>http://icbcbicc.github.io/</id><title>Jam&#39;s Blog</title><subtitle>Write your site description here. It will be used as your sites meta description as well!</subtitle><entry><title>Matlab数值计算</title><link href="http://icbcbicc.github.io/2016/11/13/Matlab_Numerical_Methods/" rel="alternate" type="text/html" title="Matlab数值计算" /><published>2016-11-13T00:00:00+08:00</published><updated>2016-11-13T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/11/13/Matlab_Numerical_Methods</id><content type="html" xml:base="http://icbcbicc.github.io/2016/11/13/Matlab_Numerical_Methods/">&lt;h2 id=&quot;matlab&quot;&gt;Matlab数值计算&lt;/h2&gt;

&lt;h3 id=&quot;matlab-1&quot;&gt;1. Matlab常用函数补充&lt;/h3&gt;

&lt;p&gt;其他&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt; ... &lt;/code&gt; 表示一行未结束，换行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据结构&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cell&lt;/code&gt;创建元胞，访问元胞用&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt;创建结构体，它是有名字的元胞。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;矩阵&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;A(end)&lt;/code&gt; 可以访问数组最后的一个元素。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rand&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;randn&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;randi&lt;/code&gt; 产生的随机数分别为均匀随机分布、正态随机分布、均匀随机分布整数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;diag&lt;/code&gt; 对角矩阵，&lt;code class=&quot;highlighter-rouge&quot;&gt;blkdiag&lt;/code&gt; 对角块矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sqrtm&lt;/code&gt; 矩阵的主平方根。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hilb&lt;/code&gt;希尔伯特矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hadamard&lt;/code&gt;阿达马(Hadamard)矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wilkinson&lt;/code&gt;威尔金森(Wilkinson)矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gallery&lt;/code&gt;可用于生成多种特殊的矩阵。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;字符串&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;findstr&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;strrep&lt;/code&gt; 分别为字符串种查找字串、替换字串。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2维绘图&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fplot&lt;/code&gt; 画图时自动确定采样频率。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;y=@(t) sin(x.^3)&lt;/code&gt; 定义了一个匿名函数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;xlim&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;ylim&lt;/code&gt; 限制了坐标轴的显示范围，常用于看图像的局部。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3维绘图&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;meshgrid&lt;/code&gt; 生成一个2维的完全点集。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;surfl&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;contour&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;contourf&lt;/code&gt; 分别表示曲面、三维等高线、2维填充等高线。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section&quot;&gt;2. 线性方程组和特征系统&lt;/h3&gt;

&lt;p&gt;线性方程组$Ax=b$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;可用&lt;code class=&quot;highlighter-rouge&quot;&gt;x = A\b&lt;/code&gt;求解，相当于&lt;code class=&quot;highlighter-rouge&quot;&gt;x=b/A&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;x=inv(A)*b&lt;/code&gt;。&lt;code class=&quot;highlighter-rouge&quot;&gt;\&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt;运算符会根据矩阵的特征采用不同的算法，可以求欠定、正定、超定等等各种方程。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;利用&lt;code class=&quot;highlighter-rouge&quot;&gt;inv(A) = adj(A) / |A|&lt;/code&gt;来求逆的效率很低，尽量少用。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;b=0&lt;/code&gt; 则为 &lt;strong&gt;齐次&lt;/strong&gt; 方程组。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;|A| = 0&lt;/code&gt; 表示矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;是 &lt;strong&gt;奇异&lt;/strong&gt; 的，他的逆矩阵不存在，此时方程组无解或者有无穷多解。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rank(A)&lt;/code&gt;表示矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;的 &lt;strong&gt;秩&lt;/strong&gt;，它代表了矩阵中线性无关的行(或列)的个数，同时也是 &lt;strong&gt;约化阶梯矩阵&lt;/strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rref(A)&lt;/code&gt;非零行的个数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当&lt;code class=&quot;highlighter-rouge&quot;&gt;size(A) = [m,n]&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;rank(A) = min(m,n)&lt;/code&gt;表示&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;&lt;strong&gt;满秩&lt;/strong&gt;，也就是方程有唯一解。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;超定&lt;/strong&gt;：方程个数大于未知数个数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;病态方程组&lt;/strong&gt; 是指&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;中元素微小的变化都会导致解的巨大变化，在近似解时会很大的影响精度。
可用&lt;code class=&quot;highlighter-rouge&quot;&gt;cond&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;rcond&lt;/code&gt;来判断矩阵的良态。
前者的范围是 1 到正无穷，1 表示完美良态。
后者从 0 到 1 ，0 表示完美良态。&lt;code class=&quot;highlighter-rouge&quot;&gt;rcond&lt;/code&gt;的计算比较不准确，但速度快。
希尔伯特矩阵是非常病态的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;不是方阵，无法求逆。可以使用 &lt;strong&gt;伪逆&lt;/strong&gt;$A^+$&lt;br /&gt;
$A^+=(A^TA)^{-1}A^T \quad m&amp;gt;n$&lt;br /&gt;
$A^+=A^T(AA^T)^{-1} \quad m&amp;lt;n$，此公式要求A是满秩的。&lt;br /&gt;
当然不满秩也能求，$A^+=VS^TSV^T \quad s.t. \quad [U,S,V]= svd(A)$。&lt;br /&gt;
使用&lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt;即可自动求伪逆而不用关心是否满秩和m、n的大小。&lt;br /&gt;
伪逆常用于求解超定或者欠定方程组，但其实&lt;code class=&quot;highlighter-rouge&quot;&gt;\&lt;/code&gt;符号已经包括了这个功能。&lt;br /&gt;
要求非负的解可用&lt;code class=&quot;highlighter-rouge&quot;&gt;lsqnonneg&lt;/code&gt;，这是一个非负的最小二乘问题。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;稀疏矩阵&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sparse(A)&lt;/code&gt;将&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;转化为稀疏形式，当然前提是&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;本身是稀疏的。&lt;code class=&quot;highlighter-rouge&quot;&gt;full&lt;/code&gt;是逆过程。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;issparse&lt;/code&gt;可判断函数是否稀疏。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nnz&lt;/code&gt;给出矩阵中非0元素个数。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sprandn&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;sprandsys&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;sprandn&lt;/code&gt;分别生成稀疏正态分布随机矩阵、稀疏随机对称矩阵、稀疏正态分布随机整数矩阵。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;利用稀疏矩阵能极大加速运算。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;eig&lt;/code&gt;用于求特征值。&lt;code class=&quot;highlighter-rouge&quot;&gt;eigs&lt;/code&gt;求部分特征值。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;3. 非线性方程组的解&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;二分法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;不动点（迭代）法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;牛顿法 &lt;code class=&quot;highlighter-rouge&quot;&gt;fnewton&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;贝尔斯托法 &lt;code class=&quot;highlighter-rouge&quot;&gt;bairstow&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;roots&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;施罗德法 &lt;code class=&quot;highlighter-rouge&quot;&gt;schroder&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;布罗伊登法 &lt;code class=&quot;highlighter-rouge&quot;&gt;broyden&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fzero&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TODO：各种方法的适用场景&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;4. 函数拟合&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>icbcbicc</name></author><category term="matlab" /><summary>Matlab数值计算</summary></entry><entry><title>The DCP Ruleset</title><link href="http://icbcbicc.github.io/2016/11/02/DCP_ruleset/" rel="alternate" type="text/html" title="The DCP Ruleset" /><published>2016-11-02T00:00:00+08:00</published><updated>2016-11-02T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/11/02/DCP_ruleset</id><content type="html" xml:base="http://icbcbicc.github.io/2016/11/02/DCP_ruleset/">&lt;h2 id=&quot;the-dcp-ruleset&quot;&gt;The DCP Ruleset&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;1. 函数的分类&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;常量&lt;/li&gt;
  &lt;li&gt;仿射&lt;/li&gt;
  &lt;li&gt;凸&lt;/li&gt;
  &lt;li&gt;凹&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;定义：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/13.JPG&quot; alt=&quot;A taxonomy of curvature&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这几个分类是有&lt;strong&gt;重叠&lt;/strong&gt;的：比如常量属于仿射，仿射既属于凸也属于凹。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2. 最主要的规则&lt;/h3&gt;

&lt;p&gt;CVX支持以下3种DCP&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;最小化问题
    &lt;ol&gt;
      &lt;li&gt;目标函数为凸&lt;/li&gt;
      &lt;li&gt;约束条件可以有任意个&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最大化问题
 	1. 目标函数为凹
 	2. 约束条件可以有任意个&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;可行性问题
 	1. 没有目标函数
 	2. 有1个或以上的约束条件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;3. 约束条件&lt;/h3&gt;

&lt;p&gt;CVX支持以下3种约束条件&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$=$ : 两侧都必须是仿射&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\preceq$：左侧为凸，右侧为凹&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\succeq$：左侧为凹，右侧为凸&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;3.1 具体要求&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;不支持&lt;code class=&quot;highlighter-rouge&quot;&gt;!=&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;~=&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;复数
    &lt;ul&gt;
      &lt;li&gt;等式约束的两侧可以是复数，可以分解为2个实数的约束。&lt;/li&gt;
      &lt;li&gt;不等式约束的两侧不能有复数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;集合元素：集合元素的约束必须是等式，且等式两侧必须为仿射&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;严格不等（strict inequalities）
    &lt;ul&gt;
      &lt;li&gt;$\prec$&lt;/li&gt;
      &lt;li&gt;$\succ$&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;因为一些数学原理和凸优化算法的原因，CVX不能保证不等式被严格遵守，因此应该尽量不使用严格不等。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;如果模型中必须用到严格不等，可以使用以下方法 &lt;em&gt;消除严格不等&lt;/em&gt; :&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;采用 &lt;strong&gt;正规化&lt;/strong&gt; 的方法来使其符合CVX的要求。&lt;/p&gt;

    &lt;p&gt;例如：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;A x = 0, \quad C x \preceq 0, \quad x \succ 0&lt;/script&gt;

    &lt;p&gt;可以转化为&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;A x = 0, \quad C x \preceq 0, \quad x \succeq 0, \quad \mathbf{1}^T x = 1&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;添加一个 &lt;strong&gt;偏差(offset)&lt;/strong&gt; 将原式变为非严格不等。&lt;/p&gt;

    &lt;p&gt;例如:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;x &gt; 0 转化为 x \ge 1e-4&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;4. 表达式规则&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;常量表达式： 可以产生有限结果的表达式&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;仿射表达式：
    &lt;ul&gt;
      &lt;li&gt;常量表达式&lt;/li&gt;
      &lt;li&gt;已声明的变量&lt;/li&gt;
      &lt;li&gt;可产生有限结果的函数调用&lt;/li&gt;
      &lt;li&gt;仿射表达式的加减组合&lt;/li&gt;
      &lt;li&gt;常量和仿射表达式的乘积&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;凸表达式
    &lt;ul&gt;
      &lt;li&gt;常量或者仿射表达式&lt;/li&gt;
      &lt;li&gt;可产生凸结果的函数调用&lt;/li&gt;
      &lt;li&gt;仿射标量的偶数次方（不包括0次方）&lt;/li&gt;
      &lt;li&gt;2次凸标量&lt;/li&gt;
      &lt;li&gt;多个凸表达式的和&lt;/li&gt;
      &lt;li&gt;凸表达式和凹表达式的差&lt;/li&gt;
      &lt;li&gt;非负常量和凸表达式的乘积&lt;/li&gt;
      &lt;li&gt;非正常量和凹表达式的乘积&lt;/li&gt;
      &lt;li&gt;凹表达式取反&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;凹表达式
    &lt;ul&gt;
      &lt;li&gt;常量或者仿射表达式&lt;/li&gt;
      &lt;li&gt;可产生凹结果的函数调用&lt;/li&gt;
      &lt;li&gt;凹标量的$p$次方($0&amp;lt; p &amp;lt; 1$)&lt;/li&gt;
      &lt;li&gt;2次凹标量&lt;/li&gt;
      &lt;li&gt;多个凹表达式的和&lt;/li&gt;
      &lt;li&gt;凹表达式和凸表达式的差&lt;/li&gt;
      &lt;li&gt;非负常量和凹表达式的乘积&lt;/li&gt;
      &lt;li&gt;非正常量和凸表达式的乘积&lt;/li&gt;
      &lt;li&gt;凸表达式取反&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;任何不符合以上规则的表达式都会被CVX禁止，尽管它可能是凸的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;涉及到矩阵时以其中的元素作为考量。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;不支持非标量之间的乘法&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;例如： &lt;code class=&quot;highlighter-rouge&quot;&gt;x*sqrt{x}&lt;/code&gt; 不被接受，但可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;pow_p(x, 3/2)&lt;/code&gt; 替代。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;5. 函数&lt;/h3&gt;

&lt;p&gt;CVX中的函数有2个特征：曲度(curvature)和单调性。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;变量必须在函数的隐含定义域中。只需定义用户自定的定义域。
  例如，不用为添加$x\ge0$的约束。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CVX判断凹凸时不考虑隐含的定义域或者自定的定义域，在定义域外为$+\inf$的被认为是凸，反之为凹。&lt;/p&gt;

    &lt;p&gt;例如：$\frac{1}{x} \quad s.t. x \ge 1$ 不被CVX认为是凸的，尽管它事实上是。解决方法是将$x &amp;lt; 0$ 的区域的函数值定义为$+\inf$，可以用CVX的函数 &lt;code class=&quot;highlighter-rouge&quot;&gt;inv_pos(x)&lt;/code&gt; 来完成。反之， &lt;code class=&quot;highlighter-rouge&quot;&gt;-inv_pos(-x)&lt;/code&gt; 将表示凹的版本。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CVX判断单调性时不考虑定义域，比如$\sqrt{x}$在$x &amp;lt; 0$的区域任然被认为是非减的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多元函数的凹凸性是由所有变量联合决定的，但单调性可以按每个变量来讨论。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;有些多元函数仅对一部分参数是凸的，凹的，仿射的，此时其他参数必须设为常数。比如：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;norm(x, p) \quad s.t. \quad p \ge 1&lt;/script&gt;

    &lt;p&gt;仅仅对$x$是凸的，因此在CVX中$p$必须设为常数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;函数嵌套：&lt;/p&gt;

    &lt;p&gt;凸函数、凹函数、仿射函数可以接受一个仿射表达式，结果相应为凸、凹、仿射的。&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;当已知 &lt;em&gt;函数为凸&lt;/em&gt; 和函数对于各个参数的单调性时：&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;函数对于某参数不减：这个参数必须是凸的&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;函数对于某参数不增：这个参数必须是凹的&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;函数对于某参数不减不增：这个参数必须是仿射的&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;当每个参数都满足以上条件，这个函数就是凸的并且可以被CVX接受。函数为凹时第1，2条相反，第3条相同。&lt;/p&gt;

    &lt;p&gt;例如：&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;max(abs(x))&lt;/code&gt;，其中&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;时向量。&lt;code class=&quot;highlighter-rouge&quot;&gt;max&lt;/code&gt;函数是凸的并且对于任一参数不减，并且&lt;code class=&quot;highlighter-rouge&quot;&gt;abs&lt;/code&gt;函数也是凸的，因此整体是凸的。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum(sqrt(x))&lt;/code&gt;，因为&lt;code class=&quot;highlighter-rouge&quot;&gt;sum&lt;/code&gt;函数是仿射且不减的，而&lt;code class=&quot;highlighter-rouge&quot;&gt;sqrt&lt;/code&gt;是凹的，因此整体为凹。同理&lt;code class=&quot;highlighter-rouge&quot;&gt;sum(square(x))&lt;/code&gt;是凸的。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;非线性嵌套的单调性&lt;/p&gt;

    &lt;p&gt;因为很多凸函数在定义域上没有单调性，因此不能直接使用（自行定义定义域也不行）。CVX的很多函数有2种形式，一种是原始的，一种是在固定定义域有单调性的。例如：
  &lt;code class=&quot;highlighter-rouge&quot;&gt;square_pos()&lt;/code&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_pos()&lt;/code&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;quad_pos_over_lin()&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;关于平方&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;事实上，CVX原本并不支持平方表达式，但CVX将自动识别以下几种边的表达式&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$x . * x$&lt;/li&gt;
  &lt;li&gt;$conj( x ) . * x$&lt;/li&gt;
  &lt;li&gt;$y’ * y$&lt;/li&gt;
  &lt;li&gt;$(A * x - b)’ * Q * (Ax-b)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;并将其转为相应的可接受的形式：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_abs( y )&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_abs( y )&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sum_square_abs( y )&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;quad_form( A * x - b, Q )&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CVX建议尽量少用平方，因为平方是平滑的，它一般是作为非平滑的真实目标函数的替代，然而CVX本身支持很多的非平滑函数。因此使用非平方可以更加精确。例如： &lt;code class=&quot;highlighter-rouge&quot;&gt;sum( ( A * x - b ) . ^ 2 ) &amp;lt;= 1&lt;/code&gt; 可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;norm( A * x - b )&amp;lt;= 1&lt;/code&gt; 替代，也就是欧式范数。&lt;/p&gt;</content><author><name>icbcbicc</name></author><category term="convex optimization" /><summary>The DCP Ruleset</summary></entry><entry><title>Sparse Reconstruction Cost for Abnormal Event Detection</title><link href="http://icbcbicc.github.io/2016/10/23/Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection/" rel="alternate" type="text/html" title="Sparse Reconstruction Cost for Abnormal Event Detection" /><published>2016-10-23T00:00:00+08:00</published><updated>2016-10-23T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/23/Sparse Reconstruction Cost for Abnormal Event Detection</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/23/Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection/">&lt;h1 id=&quot;sparse-reconstruction-cost-for-abnormal-event-detection&quot;&gt;Sparse Reconstruction Cost for Abnormal Event Detection&lt;/h1&gt;
&lt;p&gt;Authors: &lt;em&gt;Yang Cong, Junsong Yuan, Ji Liu&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;http://www.cs.rochester.edu/~jliu/paper/Cong-Yuan-CVPR11.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;conventional-algorithms&quot;&gt;Conventional Algorithms&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Probability Model&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Detect testing sample with lower probability as anormaly by fitting a probability model to the training data. However, the required number of training data increases exponentially with the feature dimension, and it’s unrealistic to collect enough data for density estimation in practice&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sparse-reconstruction-cost-src&quot;&gt;Sparse Reconstruction Cost (SRC)&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;We propose SRC based on the weighted $l_1$ minimization.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Normal event: generate sparse reconstruction coefficients with a &lt;strong&gt;small SRC&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abnormal events: generate a dense representation with a &lt;strong&gt;large SRC&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;types-of-abnormal-events&quot;&gt;2 types of abnormal events:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Local abnormal event (LAE)&lt;/p&gt;

    &lt;p&gt;The local behavior is different from its spatio temporal neighborhoods&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Global abnormal event (GAE)&lt;/p&gt;

    &lt;p&gt;The whole scene is abnormal, even though any individual lcoal behavior can be normal&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;New dictionary selection method&lt;/p&gt;

    &lt;p&gt;Reduce the size of the basis set $\phi$ for an efficient reconstruction&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>icbcbicc</name></author><summary>Sparse Reconstruction Cost for Abnormal Event Detection
Authors: Yang Cong, Junsong Yuan, Ji Liu
PDF</summary></entry><entry><title>Machine Learning (Zhihua Zhou) Notes 03</title><link href="http://icbcbicc.github.io/2016/10/16/machine_learning-_zhihua_zhou_notes_03/" rel="alternate" type="text/html" title="Machine Learning (Zhihua Zhou) Notes 03" /><published>2016-10-16T00:00:00+08:00</published><updated>2016-10-16T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/16/machine_learning _zhihua_zhou_notes_03</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/16/machine_learning-_zhihua_zhou_notes_03/"></content><author><name>icbcbicc</name></author><category term="machine learning" /><summary></summary></entry><entry><title>Machine Learning (Zhihua Zhou) Notes 02</title><link href="http://icbcbicc.github.io/2016/10/15/machine_learning-_zhihua_zhou_notes_02/" rel="alternate" type="text/html" title="Machine Learning (Zhihua Zhou) Notes 02" /><published>2016-10-15T00:00:00+08:00</published><updated>2016-10-15T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/15/machine_learning _zhihua_zhou_notes_02</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/15/machine_learning-_zhihua_zhou_notes_02/">&lt;h2 id=&quot;section&quot;&gt;3. 线性模型&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;3.2 线性回归&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基本概念&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;线性回归：用线性模型拟合目标函数。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;均方误差：线性回归中常用的性能度量，它的几何意义是欧几里得距离(Euclidean distance)。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;最小二乘法(least square method)：基于均方误差最小化来求解模型的方法。几何意义是在目标空间中找到一个超平面，使样本点到该平面的欧式距离之和最小。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多元线性回归(multivariate linear regression)&lt;/p&gt;

    &lt;p&gt;求最优$\hat{\omega}$使目标函数$E_\hat{\omega}$最小:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;E_\hat{\omega} = (y-X\hat{\omega})^T(y-X\hat{\omega})&lt;/script&gt;

    &lt;p&gt;对$\hat{\omega}$求导：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}} = 2X^T(X\hat{\omega}-y)&lt;/script&gt;

    &lt;p&gt;当$X^TX$是满秩矩阵(full-rank matrix)或正定矩阵(positive definite matrix)时，可直接求得最优解：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\omega} = (X^TX)^{-1}X^Ty&lt;/script&gt;

    &lt;p&gt;最终模型：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_i) = x_i^T(X^TX)^{-1}X^Ty&lt;/script&gt;

    &lt;p&gt;然而当$X^TX$不是满秩矩阵时，$X$的列数多于行数，也就是特征数比样本数多。此时方程有多个最优解，需要引入正则化(regularization)来选择一个$\hat{\omega}$。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;广义线性模型(generalized linear model)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;y = g^{-1}(\omega^Tx+b)&lt;/script&gt;

    &lt;p&gt;其中，$g(.)$是单调可微函数。称之为联系函数(link function)&lt;/p&gt;

    &lt;p&gt;较长用的联系函数是$ln(.)$，此时称之为对数线性回归(log-linear regression)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对数几率回归(logistic regression)&lt;/p&gt;

    &lt;p&gt;用于分类任务，将输出映射到${0,1}$的集合上。&lt;/p&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y = \frac{1}{1+e^{-(\omega^Tx+b)}}&lt;/script&gt;
  $\to$
  &lt;script type=&quot;math/tex&quot;&gt;ln\frac{y}{1-y} = \omega^Tx+b&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;若将$y$视为取正例的概率，则$1-y$为取反例的概率。&lt;/p&gt;

    &lt;p&gt;$\frac{y}{1-y}$称为&lt;strong&gt;几率(odds)&lt;/strong&gt;，反映了相对可能性。&lt;/p&gt;

    &lt;p&gt;$ln\frac{y}{1-y}$称为&lt;strong&gt;对数几率(logit)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;可通过&lt;strong&gt;极大似然法(maximum likeihood method)&lt;/strong&gt;估计最优解：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\omega,b) = \sum_{i=1}^{m}ln\{p(y_i|x_i;\omega;b)\}&lt;/script&gt;

    &lt;p&gt;$=$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{m}ln\{y_ip(y=1|x_i;\omega;b)+(1-y_i)p(y=0|x_i;\omega;b)\}&lt;/script&gt;

    &lt;p&gt;$=$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{m}(-y_i(\omega;b)^Tx_i+ln(1+e^{(\omega;b)^T}x_i))&lt;/script&gt;

    &lt;p&gt;此式为关于$(\omega;b)$高阶可导连续凸函数。可用&lt;strong&gt;梯度下降、牛顿法&lt;/strong&gt;进行求解。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-discriminant-analysisldafisher&quot;&gt;3.4 线性判别分析(Linear Discriminant Analysis，LDA，亦称Fisher判别分析)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;主要思想：将共有$N$类的样本投影到一个$N-1$维的空间，使同类的投影点接近，不同类的投影点相互远离。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;指标&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$X_i,\mu_i,\Sigma_i$分别为第$i$类的样本点、均值向量、协方差矩阵。$\mu$为所有样本的均值向量。假设共$N$类，第$i$类有$m_i$个样本，共$m$个样本。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;全局散度矩阵：&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_t = S_b+S_w = \sum_{i=1}^m (x_i-\mu)(x_i-\mu)^T&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;类内散度矩阵：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_w = \sum_{i=1}^N S_{w_i} = \sum_{i=1}^N \sum_{x\in X_i}(x-\mu)(x-\mu)^T&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;类间散度矩阵：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_b = S_t-S_w = \sum_{i=1}^N m_i(\mu_i-\mu)(\mu_i-\mu)^T&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;优化：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;可使用$S_t,S_w,S_b$中的任意2个指标进行优化。常用的优化目标是：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;max\frac{tr(W^TS_bW)}{tr(W^TS_wW)} \quad s.t. \quad W \in R^{d \times (N-1)}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;此式可以通过&lt;strong&gt;拉格朗日乘子法&lt;/strong&gt;转化为广义特征值问题：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_bW = \lambda S_wW&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$W$的闭式解：&lt;strong&gt;$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量&lt;/strong&gt;组成的矩阵。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;3.5 多分类学习&lt;/h3&gt;

&lt;p&gt;将多分类任务拆分为多个2分类任务&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;拆分策略&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;一对一(One vs. One, OvO)：共$N(N-1)/2$个分类器&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;一对多(One vs. Rest, OvR)：共$N$个分类器。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多对多(Many vs. Many, MvM)&lt;/p&gt;

    &lt;p&gt;用、纠错输出码(Error Correcting Output Codes, EOOC)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;对数据进行$M$次划分，得到$M$组{训练集，测试集}，从而得到$M$个分类器。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;对每个样本，分别用$M$个分类器分类，这些预测标记组成一个编码序列。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;将预测编码与每个类别自己的编码(One-hot码)比较，距离(海明距离，欧氏距离)最近的为最终预测结果。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;EOOC对与分类器的错误有一定容忍和修正能力。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;3.6 类别不平衡问题&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;再缩放(rescaling)：$\frac{y}{1-y}&amp;gt;\frac{m^+}{m+-}$：预测为正例。其中$m^+,m^-$分别为正例、反例的数目。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;欠采样(undersampling):去除一部分样本使得样本平衡&lt;/p&gt;

    &lt;p&gt;将类别较多的样本划分为几个集合，形成多组{训练集，测试集}分别学习。虽然每组是欠采样，但全局上却没有欠采样，充分利用了数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;过采样(oversampling):增加一部分样本使得样本平衡&lt;/p&gt;

    &lt;p&gt;不能直接对原有样本进行重复采样，否则将会出现严重的过拟合。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;4.决策树&lt;/h2&gt;

&lt;p&gt;TODO：信息增益、增益率、基尼指数、剪枝、连续值处理、多变量决策树（斜划分）&lt;/p&gt;</content><author><name>icbcbicc</name></author><category term="machine learning" /><summary>3. 线性模型</summary></entry><entry><title>Machine Learning (Zhihua Zhou) Notes 01</title><link href="http://icbcbicc.github.io/2016/10/14/machine_learning_zhihua_zhou_notes_01/" rel="alternate" type="text/html" title="Machine Learning (Zhihua Zhou) Notes 01" /><published>2016-10-14T00:00:00+08:00</published><updated>2016-10-14T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/14/machine_learning_zhihua_zhou_notes_01</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/14/machine_learning_zhihua_zhou_notes_01/">&lt;h1 id=&quot;section&quot;&gt;周志华《机器学习》笔记-01&lt;/h1&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 模型的评估与选择&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.1 经验误差与过拟合&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;三种误差&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;训练误差(training error)、经验误差(empirical error)&lt;/strong&gt;：训练集上的误差，受到欠拟合与过拟合的影响&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;测试误差(testing error)&lt;/strong&gt;：测试集上的误差，通过常作为泛化误差的近似&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;泛化误差(generalization error)&lt;/strong&gt;：在新样本上的误差，无法直接获得&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;欠拟合与过拟合&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;欠拟合易于克服：增加训练轮数&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;过拟合是机器学习面临的关键障碍，无法彻底避免。因为机器学习面临的问题通常是$NP$难甚至更难。然而学习算法必然要在多项式时间完成，要想彻底避免过拟合，就要通过误差最小化直接获得最优解，也就意味着$P=NP$。只要坚信$P\ne NP$，过拟合就不可避免。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.2 产生测试集的方法&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2.2.1 留出法(hold-out)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;方法：将数据集分为互斥的两个集合。采用分层采样(stratified sampling)来保证2个集合的数据分布一致性（也就是训练集和测试集中相同类型样本所占的比例相同）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缺陷：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;由于单次使用留出法得到的结果不够可靠（即使保证相同类别样本比例相同，仍然有多种方式划分），一般采用多次随机划分得到多组{训练集，测试集}进行训练，误差取平均的方式。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;训练集占的多：模型精确，但评估结果不够稳定准确；测试集占的多，模型精度不足。此问题没有完美解决方案，一般选取$\frac{2}{3}$到$\frac{4}{5}$的样本用于训练。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.2.2 交叉验证法(cross validation)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;方法：
    &lt;ol&gt;
      &lt;li&gt;先将数据划为$k$个大小相同，数据分布相同的互斥子集&lt;/li&gt;
      &lt;li&gt;每次使用$k-1$个子集进行训练，$1$个做测试。一共可获得$k$组{训练集，测试集}的组合。&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;关键：结果的稳定性和真实性很大程度上取决于$k$的选择，因此这个方法被称作“$k$折交叉验证($k$-fold cross validation)”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缺陷：与留出法相似，将数据划分为$k$组有很多方式，产生了不确定因素。一般采用进行$p$次划分取均值的方法，也就是“$p$次$k$折交叉验证”，一共得到$k \times p$组{训练集，测试集}。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;特例：当$k=样本数$，称为“留一法(Leave-One-Out，LOO)”。此方式不受样本划分的影响，评估较准确，但计算量太大。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.2.3 自助法(bootstrapping)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;以自助采样法(bootstrapping sampling，亦称“可重复采样/有放回采样”)为基础，&lt;strong&gt;适合数据集小的情况&lt;/strong&gt; 。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;方法：在数据集$D$中有放回的随机取出一个样本，重复$m$次,得到$D’$作为训练集，$D-D’$作为测试集。$D’$中可能有重复的样本。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当$m$足够大时，一个样本在$m$次选取中不被选中的概率是
&lt;script type=&quot;math/tex&quot;&gt;$\lim_{m \to \inf}(1-\frac{1}{m})^m \to \frac{1}{e} \approx 0.368&lt;/script&gt;$
因此$D$中有$36.8\%$的样本不在$D’$中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缺陷：改变了初始数据的分布。数据量足够时一般不采用此方法。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.2.4 调参与最终模型&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;两类参数：
    &lt;ol&gt;
      &lt;li&gt;超参数：人工设定算法的参数，数目较少&lt;/li&gt;
      &lt;li&gt;模型参数：由算法生成的参数，数量可以很多&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;在模型最优化后，应该使用所有数据（包括测试集）再训练一次，得到最终模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;2.3 性能度量&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2.3.2 查准率，查全率与$F$度量&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;混淆矩阵(confusion matrix)：&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt; &lt;/th&gt;
          &lt;th&gt; &lt;/th&gt;
          &lt;th&gt;预测结果&lt;/th&gt;
          &lt;th&gt; &lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;正例&lt;/td&gt;
          &lt;td&gt;反例&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;真实情况&lt;/td&gt;
          &lt;td&gt;正例&lt;/td&gt;
          &lt;td&gt;TP&lt;/td&gt;
          &lt;td&gt;FN&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;反例&lt;/td&gt;
          &lt;td&gt;FP&lt;/td&gt;
          &lt;td&gt;TN&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查准率(precision，亦称准确率)：$P = \frac{TP}{TP+FP}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查全率(recall，亦称召回率)：$R = \frac{TP}{TP+FN}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;查全率与查准率通常是矛盾的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;P-R曲线
    &lt;ul&gt;
      &lt;li&gt;方法：
        &lt;ol&gt;
          &lt;li&gt;将测试样本按概率从大到小排序&lt;/li&gt;
          &lt;li&gt;将前$n$个样本作为正例，得到查准率与查全率&lt;/li&gt;
          &lt;li&gt;$n = n+1$&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;利用P-R曲线的评估：
  &lt;img src=&quot;/img/13.png&quot; alt=&quot;P-R curve&quot; /&gt;
        &lt;ol&gt;
          &lt;li&gt;曲线下面积越大，则该模型越好。当一个曲线包住了另一个曲线而没有相交，则认为外面的曲线表示的模型更好（比如下图中的蓝色比绿色好）。&lt;/li&gt;
          &lt;li&gt;$P = R$的点称为平衡点(Break-Even Point, BEP)，也就是曲线与对角线的交点，也可作为评估依据：越大越到。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$F_1$与$F_\beta$度量&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;F1 = \frac{2 \times P \times R}{P+R}&lt;/script&gt;
  实际上就是P与R的调和平均。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;F_\beta = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}&lt;/script&gt;
  实际上就是P与R的加权调和平均。$\beta$决定了查全率和查准率谁更重要：$\beta &amp;gt; 1$时，查全率更重要；$0 &amp;lt; \beta &amp;lt; 1$时，查准率更重要。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;宏查准率，宏查全率，宏$F1$；微查准率，微查全率，微$F1$：略&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.3.3 ROC与AUC&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;截断点(cut point)的选择：&lt;/p&gt;

    &lt;p&gt;将样本按概率从大到小排序&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;查准率高：截断点靠前&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;查全率高：截断点靠后&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ROC(Receiver Operating Characteristic，受试者工作特征)
  &lt;img src=&quot;/img/14.png&quot; alt=&quot;ROC curve&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;纵轴：真正率
  &lt;script type=&quot;math/tex&quot;&gt;TPR = \frac{TP}{TP+FN}&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;横轴：假正率
  &lt;script type=&quot;math/tex&quot;&gt;FPR = \frac{FP}{TN+FP}&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;对角线：随机猜测模型&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;评估方式：与P-R曲线相同。曲线下的面积称为AUC（Aera under ROC）&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.3.4 代价敏感错误率与代价曲线&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;之前的评价方式隐式假设了均等代价，即将错误个数作为代价。在非均等代价下，ROC曲线不能反映出总体代价，此时应该使用代价曲线。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;代价曲线(cost surve)&lt;/strong&gt;
  &lt;img src=&quot;/img/15.png&quot; alt=&quot;cost curve&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;横轴：正例概率代价
  &lt;script type=&quot;math/tex&quot;&gt;P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01}+(1-p) \times cost_{10}}&lt;/script&gt;
其中，$p$为正例的概率；$cost_{01}$为假反率代价；$cost_{10}$为假正率代价。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;纵轴：归一化代价
  &lt;script type=&quot;math/tex&quot;&gt;cost_{norm} = \frac{(1-TPR) \times p \times cost_{01}+FPR \times (1-p) \times cost_{10}}{p \times cost_{01}+(1-p) \times cost_{10}}&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;在ROC曲线上取一点(FPR, TPR)，在代价曲线上将左纵轴上的(0,FPR)与右纵轴上的(1，1-TPR)2点连成线段。所有线段围成的部分面积为&lt;strong&gt;期望总体代价&lt;/strong&gt;。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;2.4 比较检验&lt;/h3&gt;
&lt;p&gt;用于度量模型在测试集上的性能有多大概率是泛化性能，暂略&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.4.1 假设检验&lt;/strong&gt;
&lt;strong&gt;2.4.2 价差验证t检验&lt;/strong&gt;
&lt;strong&gt;2.4.3 McNemar检验&lt;/strong&gt;
&lt;strong&gt;2.4.4 Friedman检验与Nemenyi后续检验&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;2.5 偏差与方差&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;基于均方误差的回归任务的泛化误差可分解为偏差、方差、噪声之和&lt;/strong&gt;。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;偏差：标记与真实类别的偏差。表明了在当前任务上任何学习算法所能达到的期望泛化误差的最小值，是任务本身的属性。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;方差：同样大小的训练集变化所导致的性能变化，即数据扰动造成的影响。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;偏差：算法本身的拟合能力。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;偏差-方差窘境(bias-variance dilemma)&lt;/p&gt;

    &lt;p&gt;偏差与方差一般都是冲突的。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;训练刚开始：拟合能力不足，受数据扰动的影响较小。偏差主导了泛化错误率。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;训练充足：拟合能力很强，受数据扰动的影响大。方差主导了泛化错误率。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attention：对于分类任务，由于0/1损失函数的跳变性，偏差-方差分解很困难。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>icbcbicc</name></author><category term="machine learning" /><summary>周志华《机器学习》笔记-01</summary></entry><entry><title>Online Detection of Abnormal Events Using Incremental Coding Length</title><link href="http://icbcbicc.github.io/2016/10/04/Online-Detection-of-Abnormal-Events-Using-Incremental-Coding-Length/" rel="alternate" type="text/html" title="Online Detection of Abnormal Events Using Incremental Coding Length" /><published>2016-10-04T00:00:00+08:00</published><updated>2016-10-04T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/04/Online Detection of Abnormal Events Using Incremental Coding Length</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/04/Online-Detection-of-Abnormal-Events-Using-Incremental-Coding-Length/">&lt;h1 id=&quot;online-detection-of-abnormal-events-using-incremental-coding-length&quot;&gt;Online Detection of Abnormal Events Using Incremental Coding Length&lt;/h1&gt;

&lt;p&gt;Authors: &lt;em&gt;Jatanta K.Dutta, Bonny Banerjee&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/83f6/d84389dcdc25a4a1462044d7b1cbc2e75eac.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;本文使用稀疏编码进行异常事件检测。数据预处理、特征提取、稀疏编码、字典生成都使用传统方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创新之处在于引进了 Incremental Coding Length (ICL)作为稀疏表达的评价，它代表了每个特征的熵的增加量。异常事件可以由特征的rarity来定义（罕见的特征意味着异常）。最终，将所有特征的energy按权重相加就可以判断是否有异常了。文中将每一个特征的energy作为rarity， 它是关于ICL的函数。ICL的计算不需要任何参数，也不需要关于数据的先验假设&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;1. Abstract&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;事件的异常主要是由2个因素同时决定的:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在稀疏表达事件的过程中，每个特征使用的频率&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;稀疏表达中特征的系数&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;2. Introduction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;异常事件的检测面临3个难点：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;无监督的2分类&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;数据量大，无法全部存储&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;数据的分布不恒定&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;3. Related Work&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;异常事件检测通常由以下几个步骤组成（以及其常用方法）：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;数据预处理，获得低级表达&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Histogram of optical flow (HOF)&lt;/li&gt;
      &lt;li&gt;Muti-scale histogram of optical flow (MHOF)&lt;/li&gt;
      &lt;li&gt;Histogram of optical flow orientation (HOFO)&lt;/li&gt;
      &lt;li&gt;Spatio-temporal gradient&lt;/li&gt;
      &lt;li&gt;3D Spatio-temporal foreground mask&lt;/li&gt;
      &lt;li&gt;Binary features&lt;/li&gt;
      &lt;li&gt;Backgroud segregation (Foreground detection)&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;Mixure of optical flow (MPPCA, temporal features)&lt;/li&gt;
      &lt;li&gt;MPPCA+SF&lt;/li&gt;
      &lt;li&gt;Mixtures of dynamic textures (MDT, spatial and temporal features)&lt;/li&gt;
      &lt;li&gt;Chaotic invariant&lt;/li&gt;
      &lt;li&gt;Social force model (SF, spatial features)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对低级表达进行抽象，得到中级表达
    &lt;ul&gt;
      &lt;li&gt;Sparse coding
        &lt;ol&gt;
          &lt;li&gt;Low rank dictoinary (SparseLR)&lt;/li&gt;
          &lt;li&gt;Compact regularization (SparseCR)&lt;/li&gt;
          &lt;li&gt;LR+CR&lt;/li&gt;
          &lt;li&gt;Weighted sparse representation (SparseW)&lt;/li&gt;
          &lt;li&gt;Sparse combination learning (SCL, 150fps matlab)&lt;/li&gt;
          &lt;li&gt;Large scale dictionary selection (LSDS)&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;Hiden Markov model (HMM)&lt;/li&gt;
      &lt;li&gt;Markov random field (MRF, Saligrama)&lt;/li&gt;
      &lt;li&gt;Mixture of probabilistic principle component analysis(MPPCA)&lt;/li&gt;
      &lt;li&gt;Dimensionality reduction(PCA, ICA, clustering)&lt;/li&gt;
      &lt;li&gt;Gaussian mixture model&lt;/li&gt;
      &lt;li&gt;Latent Dirichlet allocation&lt;/li&gt;
      &lt;li&gt;Deep learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对这些表达进行评估，检测出异常（本文所关注的重点）
    &lt;ul&gt;
      &lt;li&gt;Sparse rconstruction error (SRC)&lt;/li&gt;
      &lt;li&gt;Incremental coding lenth (ICL, entropy gain)&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
      &lt;li&gt;Prediction error&lt;/li&gt;
      &lt;li&gt;Rarity index&lt;/li&gt;
      &lt;li&gt;Information content&lt;/li&gt;
      &lt;li&gt;Density-based scoring&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;按使用场景分类 (TODO)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Crowded scenario
    &lt;ul&gt;
      &lt;li&gt;Binary features based on back ground model&lt;/li&gt;
      &lt;li&gt;3D Spatio-temporal foreground mask fusing Markov Random Field&lt;/li&gt;
      &lt;li&gt;Trajectory-based approaches&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Uncrowded scenario
    &lt;ul&gt;
      &lt;li&gt;Local abnormal event&lt;/li&gt;
      &lt;li&gt;Global abnormal event&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-framework&quot;&gt;4. Proposed Framework&lt;/h2&gt;

&lt;h3 id=&quot;video-representation&quot;&gt;4.1 Video Representation&lt;/h3&gt;

&lt;p&gt;Spatiotemporal interest point detector&lt;/p&gt;

&lt;h3 id=&quot;online-sparse-dictionary-learning&quot;&gt;4.3 Online Sparse Dictionary Learning&lt;/h3&gt;

&lt;p&gt;Batch Orthogonal Matching Pursuit （Batch OMP）&lt;/p&gt;

&lt;h3 id=&quot;abnoraml-event-detection&quot;&gt;4.3 Abnoraml Event Detection&lt;/h3&gt;

&lt;p&gt;假设对某一时刻的输入数据$X(t) = [x_1(t), …, x_n(t)]$，都有稀疏系数 $\Gamma = [\gamma_1, … , \gamma_n] \in R^{k*n}$&lt;/p&gt;

&lt;p&gt;那么在时刻$t$，对于第 $j$ 个特征的 activity ratio 定义为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j(t) = \frac{\sum_{h=1}^n|\Gamma_{j,h}(t)|}{\sum_{i=1}^k\sum_{h=1}^n|\Gamma_{i,h}(t)|}&lt;/script&gt;

&lt;p&gt;$t$ 时刻的 summary activity ratio 将按照以下方式更新（初始值为$1/k$）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(t) = (1-\alpha(t))q(t-1)+\alpha(t)p(t)&lt;/script&gt;

&lt;p&gt;其中，$\alpha(t)$ 是一个时间的函数。当 $\alpha(t) = 1/t$ , $q(t)$ 就是从开始到现在的平均 activity ratio。当$\alpha(t) = 1/t_1$时， （$t_1$ 是一个正的常数）, $q(t)$ 就是前 $t_1$ 个 activity ratio 的平均值。$\alpha(t) = 1/t_1$ 对于数据分布不稳定的情况很有用。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ICL(q_j) = \frac{\partial {H(q)} }{\partial {q_j} } = -H(q) - q_j - log q_j - q_j log q_j&lt;/script&gt;

&lt;p&gt;在计算完 ICL 后，就得到了任意 $t$ 时刻的显著特征集（salient feature set）&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = \{j | ICL(q_j(t)) &gt; 0\}&lt;/script&gt;

&lt;p&gt;$S(t)$ 中的每一个显著特征的energy表示为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j(t) = \frac{ICL(q_j(t))}{\sum_{i \in S(t)}ICL(q_i(t))} \quad s.t. \quad j \in S(t)&lt;/script&gt;

&lt;p&gt;对于不属于$S(t)$的特征：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j(t) = 0 \quad s.t. \quad j \notin S(t)&lt;/script&gt;

&lt;p&gt;$\theta_j(t)$ 表示了用$j$特征来表示输入的罕见程度&lt;/p&gt;

&lt;p&gt;最终，每一个cube的 anomaly score 定义为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g = |\gamma|^T\theta&lt;/script&gt;

&lt;p&gt;将连续几帧的所有 cube 的 anomaly score 经过 gaussian filter 处理作为每一帧的 anomaly map&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experimental-result&quot;&gt;5. Experimental Result&lt;/h2&gt;

&lt;p&gt;略&lt;/p&gt;</content><author><name>icbcbicc</name></author><summary>Online Detection of Abnormal Events Using Incremental Coding Length</summary></entry><entry><title>The Negative —— Part 2</title><link href="http://icbcbicc.github.io/2016/10/02/The-Negative-Part-2/" rel="alternate" type="text/html" title="The Negative —— Part 2" /><published>2016-10-02T00:00:00+08:00</published><updated>2016-10-02T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/02/The Negative Part 2</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/02/The-Negative-Part-2/">&lt;h2 id=&quot;section&quot;&gt;4. 区域选置&lt;/h2&gt;

&lt;p&gt;由于区域选置这一概念是基于其他重要概念之上的，因此我们先介绍一些基本概念。&lt;/p&gt;</content><author><name>icbcbicc</name></author><summary>4. 区域选置</summary></entry><entry><title>The Negative —— Part 1</title><link href="http://icbcbicc.github.io/2016/10/01/The-Negative-Part-1/" rel="alternate" type="text/html" title="The Negative —— Part 1" /><published>2016-10-01T00:00:00+08:00</published><updated>2016-10-01T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/10/01/The Negative Part 1</id><content type="html" xml:base="http://icbcbicc.github.io/2016/10/01/The-Negative-Part-1/">&lt;h2 id=&quot;section&quot;&gt;1. 简介&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;最终目标:&lt;/strong&gt; 视觉化&lt;/p&gt;

&lt;p&gt;（摄影的）的工艺与技术尽管很重要，但它们永远是屈从于摄影师的表达观念的。换句话说，它们是必须的而不是主要的。&lt;/p&gt;

&lt;p&gt;有一点值得注意的是：摄影的表达，或者说摄影的创意与现实是没有直接的关系的。我们不会将观察到的事物的内在价值直接复制到照片中。但我们会模仿这些价值，或者将他们呈现在它相关的感情中。很多人认为我的照片是“写实”的。实际上，他们所谓的“现实”只是他们认为自己眼中图像是准确的，这种“现实”是是绝对偏离了真正的现实的。之所以观看者会认为我的作品是写实的，是因为作品的视觉效果很合理和可信。如果将我的作品与其中所表现的事物拿出来直接对比，就会显现出巨大的视觉差异。&lt;/p&gt;

&lt;p&gt;要实现令人满意的视觉化，关键在于在底片上体现合适的信息。&lt;/p&gt;

&lt;p&gt;实现视觉化的第一步是 &lt;strong&gt;图像管理&lt;/strong&gt;，这在此系列的第一本书（&lt;em&gt;The Camera&lt;/em&gt;）中有详细阐述，它涉及到你的观念和相机的调整。在这本书中我们将要讨论 &lt;strong&gt;图像值&lt;/strong&gt; 的控制。它是通过曝光、冲洗和其他一些因素决定的。当然，因为我们的最终目标是得到相片（正像），所以我们会根据最终的正像来讨论负片的控制。第三本书（&lt;em&gt;The Print&lt;/em&gt;）将详细地阐述得到正像和放大的过程。&lt;/p&gt;

&lt;p&gt;尽管我们分不同阶段阐述了视觉化和相关技术，但在实际操作中，我们应该一开始就把整个过程当作以一个整体来考虑。摄影是一个复杂、流动的介质，它的很多因素都不是按顺序执行的，就像杂耍艺人在抛球时要保持几个球同时在空中一样。&lt;/p&gt;

&lt;p&gt;自从1940年以来，分区系统就已经应用在摄影中。它被人们支持过也被反对过。它也有很多种不同的解释方式，但不是所有的解释都是正确的，因为这些解释与感光的原理相矛盾。现在我用的分区系统的值的范围是0到10（原文使用罗马数字，为方便书写，在此使用阿拉伯数字替代），0代表纯黑，10代表纯白。实际上，我们一般使用1和9作为真实亮度的极限，这个范围包含了所有的纹理与元素。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 了解光线&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;2.1 人类的视觉与摄影&lt;/h3&gt;

&lt;p&gt;制作一副高质量的黑白影像，关键在于学会预想彩色场景转化为黑白之后的效果。因此，研究人眼感知能力与摄影捕捉图像的能力之间所存在的差异很重要。人眼所能感知的影调范围比胶片所能记录的范围大得多。人的视觉系统能够接受1000000：1的动态范围，这让人眼可以在深夜或者阳光直射之下看清纸上的字。这意味着我们能够看到20档的动态范围。然而在胶片上，只能捕捉1000：1的动态范围，也就是10档的影调范围。区域系统理论的一个主要目的就是调整胶片的动态范围。&lt;/p&gt;

&lt;p&gt;相比摄影，人眼还具有“色彩自适应”的优点——比如我们看到阴影中的一座白色的房子，虽然它呈现出灰色，但我们还是会把他看成“白色”。然而胶片就没有这种功能，它只能保持动态范围不变。运用区域系统理论进行拍摄与显影，就能使得照片中的景象和看到的一样。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;2.2 光线的性质&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;反射光&lt;/strong&gt;&lt;br /&gt;
室外场景中，大部分光线都是反射光。被摄对象的材质（光滑程度、颜色深浅）和光线的入射角度都会影响胶片对它的记录。比如白色的墙壁能够将光线反射到其他物体上从而提亮阴影，减少反差。反之，一排影调较深的树木则会吸收环境光，保持阴影不被体谅，从而增加反差。&lt;/p&gt;

    &lt;p&gt;光线都有色彩倾向，它会对反射光的强度造成一定的影响。比如，在日出或者日落时光线偏红，因此绿色物体的反射光就会减弱。同样，阴影处的光线就会偏蓝，并且减少对黄色或者红色物体的反射率。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;直射光&lt;/strong&gt;&lt;br /&gt;
当某一光源的位置相聚拍摄对象较远时，或者被摄对象相对较小时，所产生的是直射光（硬光）。这种光线会产生较强的反差（明亮的高光与边缘清晰的阴影）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;漫射光&lt;/strong&gt;&lt;br /&gt;
漫射光所产生的阴影相比直射光更加柔和甚至消失。柔光箱可以制造漫射光源，但如果它离被摄物体较远，就达不到效果。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;要注意的是，光线的性质与光线的强度没有直接的关系。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sweet Light&lt;/strong&gt;&lt;br /&gt;
这种光线只存在于日出前的半个小时到一个小时，和日落后的半个小时到一个小时。其时间长短取决于天气的情况。尽管这是一种漫射光，但他的反差范围很大。天空中最明亮的区域位于地平线附近，光线缺乏方向性，会着照亮很多阴影。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3. 分区尺标&lt;/h2&gt;

&lt;p&gt;分区尺标是区域系统的标志性元素，他也被称做灰阶或区域尺。它是由一系列连续变化的11个影调组成的，分别标记为0区到10区（纯黑到纯白）。分区尺标可以分为3个人部分：暗部区域（0，1，2）、细节区域、高光区域。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/11.JPG&quot; alt=&quot;灰阶&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以下是几个对影像特征的描述：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;影纹&lt;/strong&gt;：能感受到影调的变化，但是没有图像信息&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;微弱的细节&lt;/strong&gt;：包含信息较少&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;肌理&lt;/strong&gt;：具有一定锐度的重复的影调变化&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;清晰的细节&lt;/strong&gt;：丰富的信息&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-5&quot;&gt;3.1 暗部区域（0-1 区）&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;0区（纯黑）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于负片来说，就是没有可用密度。然而并没有一个硬性的规定，要求0区必须占多少比例。黑色为照片的动态范围建立了基础。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1区（近似于纯黑）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这一区域的影调与0区很难区分，在没有0区的对比下，很容易将其误认为0区。这一点是胶片特性曲线直线段的起点，从这个区开始，每增加一档曝光，密度也会增加一个区。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/12.JPG&quot; alt=&quot;胶片与相纸的特性曲线&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2区（有影纹的暗部）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2区是除了表现影调之外还能提供图像信息的最暗的区，它大约有5%的反射率。因为2区位于胶片特征曲线的直线段，所以它是第一个始终可以进行“选置”的区。我们可以用反射式测光表读出数据，在此基础上降低3档曝光来获得2区的曝光参数。&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;3.2 细节区域（3-7 区）&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;3区（暗部细节）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它是我们能看到清晰细节的区域中影调最深的一个。3区确定了影像的基调。3区是确认正确曝光的基本标准。因为如果在曝光时没有在底片上记录下暗部细节，那么无论怎样显影都不会呈现任何影像。因此，我们通常将3区作为暗部测光法的测量区域，也可以作为平均测光法的暗部测光区域。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4区（影调较深的中灰区域）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4区是暗部过度到亮部的起始区。它与6区这两个过渡区对于照片的反差起到了决定性的作用。如果过渡区变小，反差就会较大。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;5区（中灰区域）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它位于整个影调区域的正中间，并处在倒易律范围（2-8区）。5区能反射照射在该区域上的约18%的光线&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;6区（较浅的中灰区）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;它是中间影调向高光区过渡的起始区域。6区是高调影像的基础。当拍摄白人肖像时，可以对其手掌测光获得曝光数值&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;7区（表现高光区域中的细节）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;7区是细节区域中最亮的一个区域，有将近72%的反射率。如果图像大部分都落在这一区域，图像就会很柔美、明亮与轻盈。它被作为平均测光法的亮部测光区。在暗室印放过程中，7区也是检验影响密度是否合适的目标区。因为7区以上的区域的密度变化会受到胶片曲线肩部的影像，形成更宽的影调范围。&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;3.3 高光区域（8-10区）&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;8区（有影纹的高光区域）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;8区是最亮的仍然包含有影像信息的区域，8区以上的区域之间已经不再是线性关系。但在测光表读数的基础上增加3档曝光则会精确地落在8区。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;9区（接近纯白）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;只有将10区与9区放在一起才能区分两者。9区位于曲线地肩部终点，也就是说如果测量一张照片的9区，会达到最低密度&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;10区（纯白）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;纸基的白度决定了10区的效果。但在后期处理照片时，很容易污染纯白的相纸。比如安全灯会在相纸上形成轻微的灰雾，过度冲洗会减少氧化钡图层。化学污渍，不完全定影与纸基干燥后影调变深，都会造成照片高光区域的动态范围损失。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;center&gt;============ 接 Part 2 =============&lt;/center&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>icbcbicc</name></author><summary>1. 简介</summary></entry><entry><title>Online Learning for Matrix Factorization and Sparse Coding</title><link href="http://icbcbicc.github.io/2016/09/21/Online-Learning-for-Matrix-Factorization-and-Sparse-Coding/" rel="alternate" type="text/html" title="Online Learning for Matrix Factorization and Sparse Coding" /><published>2016-09-21T00:00:00+08:00</published><updated>2016-09-21T00:00:00+08:00</updated><id>http://icbcbicc.github.io/2016/09/21/Online Learning for Matrix Factorization and Sparse Coding</id><content type="html" xml:base="http://icbcbicc.github.io/2016/09/21/Online-Learning-for-Matrix-Factorization-and-Sparse-Coding/">&lt;h1 id=&quot;online-learning-for-matrix-factorization-and-sparse-coding&quot;&gt;Online Learning for Matrix Factorization and Sparse Coding&lt;/h1&gt;
&lt;p&gt;Authors: &lt;em&gt;Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro&lt;/em&gt;&lt;br /&gt;
&lt;a href=&quot;http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/b5dfacb7-b178-4002-823e-fc5dcfd2d641.png&quot; alt=&quot;mindmap&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Linear decompositon of a matrix using learned dictionary instead of pre-defined one&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Usage:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;low-level: image denoising, texture synthesis, audio processing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;high-level: image classification&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Differences between maxtrix factorization and PCA:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It doesn’t impose that the basis vector to be orthogonal&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Allowing more flexibility to adapt the representation to the data&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Variants matrix factorization problem:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Non-negtive matrix factorization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sparse PCA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;2. Problem Statement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Optimize the &lt;strong&gt;&lt;em&gt;empirical cost function:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;f_n(D) = \frac{1}{n}\sum_{i=1}^n l (x_i,D)&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$D\in R^{m * k}$ :dictionary&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$m &amp;lt; &amp;lt; n$: feature nums less than sample nums&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$k &amp;lt; &amp;lt; n$: atom nums of dictoinary less than sample nums&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$k &amp;gt; m$: &lt;strong&gt;&lt;em&gt;Overcomplete dictionary&lt;/em&gt;&lt;/strong&gt;: atoms more than feature nums&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$l_1$ sparse coding problem (&lt;strong&gt;&lt;em&gt;basis pursuit&lt;/em&gt;&lt;/strong&gt;):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_1(x,D) = min\frac{1}{2}{\|x-D\alpha\|}_2^2+\lambda\|\alpha\|_1 \quad s.t. \quad \alpha \in R^k&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Disadvantage:&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;There is no direct analytic link between the value of $\lambda$ and the corresponding effective sparsity $|\alpha|_0$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;To prevent $D$ from having arbitrarily large values (which would lead to arbitrarily small values of $\alpha$), it is common to &lt;strong&gt;&lt;em&gt;constrain its columns $d_1, . . . ,d_k$ to have an $l_2-norm$&lt;/em&gt;&lt;/strong&gt; less than or equal to 1&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \{D \in R^{m * k}\quad s.t. \quad \forall j=1, ... ,k \quad d_j^Td_j \le 1\}&lt;/script&gt;

    &lt;p&gt;It’s a joint optimization problem with respet to $D$ and $\alpha=[\alpha_1, …, \alpha_k] \in R^{k * n}$, which is &lt;strong&gt;&lt;em&gt;not jointly convex&lt;/em&gt;&lt;/strong&gt; but &lt;strong&gt;&lt;em&gt;convex with respect to each of the two variables&lt;/em&gt;&lt;/strong&gt; $D$ and $\alpha$ when the other one is fixed&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Optimization method:&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Alternate between the two variables&lt;/em&gt;&lt;/strong&gt;, minimizing over one while keeping the other one fixed&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Since the computation of $\alpha$ dominates the cost of each iteration in this &lt;strong&gt;&lt;em&gt;block-coordinate descent&lt;/em&gt;&lt;/strong&gt; approach, a &lt;strong&gt;&lt;em&gt;second-order optimization&lt;/em&gt;&lt;/strong&gt; technique can be used to accurately estimate $D$ at each step when $\alpha$ is fixed&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Stochastic gradient&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Its rate of convergence is very poor in conventional optimization terms, may in fact in certain settings &lt;strong&gt;&lt;em&gt;to be faster in reaching a solution with low expected cost than second-order batch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dictionary learning:&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The classical &lt;strong&gt;&lt;em&gt;projected first-order projected stochastic gradient descent&lt;/em&gt;&lt;/strong&gt; consists of a sequence of updates of $D$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;D_t = \prod [{D_{t-1}-\delta_t \nabla_C l(x_t,D_{t-1})}]&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$\nabla_C: \quad$ Orthogonal projector onto $C$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$x_t: \quad$ i.i.d samples obtained by cycling on a randomly permuted training set&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\delta_t: \quad$ learning rate(good results are obtained using  $\delta_t = \frac{a}{t+b}$ where $a$ and $b$ are chosen depended on the training data)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;online-dictionary-learning&quot;&gt;3. Online Dictionary Learning&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;algorithm-outline&quot;&gt;3.1 Algorithm Outline&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Assuming that the training set is composed of &lt;strong&gt;&lt;em&gt;i.i.d. samples&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;f_t(D) = \frac{1}{t}\sum_{i=1}^t l (x_i,D)&lt;/script&gt;

    &lt;p&gt;and&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{f_t}(D) = \frac{1}{t}\sum_{i=1}^t(\frac{1}{2}\|x_i-D\alpha_i\|_2^2+\lambda\|\alpha_i\|_1)&lt;/script&gt;

    &lt;p&gt;&lt;strong&gt;&lt;em&gt;converge almost surely to the same limit&lt;/em&gt;&lt;/strong&gt;, and thus that $\hat{f_t}$ acts as a surrogate for $f_t$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Since $\hat{f_t}$ is close to $\hat{f_{t-1}}$ for large values of $t$, so are $D_t$ and $D_{t−1}$, use $D_{t−1}$ as &lt;strong&gt;&lt;em&gt;warm restart&lt;/em&gt;&lt;/strong&gt; for computing $D_t$ can acceerate this algorithm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;sparse-coding&quot;&gt;3.2 Sparse Coding&lt;/h4&gt;

&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;$l_1$ regularized linear least-squares problem :&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_1(x,D) = min\frac{1}{2}{\|x-D\alpha\|}_2^2+\lambda\|\alpha\|_1 \quad s.t. \quad \alpha \in R^k&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Traditional method&lt;/strong&gt; (&lt;strong&gt;&lt;em&gt;Coordinate descent with soft thresholding&lt;/em&gt;&lt;/strong&gt;)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;It is efficient when the columns of the dictionary are &lt;strong&gt;&lt;em&gt;low correlated&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The columns of learned dictionaries are in general &lt;strong&gt;&lt;em&gt;highly correlated&lt;/em&gt;&lt;/strong&gt;, which make this method rather slow&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;New method&lt;/strong&gt; (&lt;strong&gt;&lt;em&gt;LARS-Lasso algorithm&lt;/em&gt;&lt;/strong&gt;)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;With an efficient Cholesky-based implementation, it is at least as fast as the one above&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;It provides the solution with a &lt;strong&gt;&lt;em&gt;higher accuracy&lt;/em&gt;&lt;/strong&gt; and being more robust since it does not require an arbitrary stopping criterion&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/9.JPG&quot; alt=&quot;Sparse Coding&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;dictionary-update&quot;&gt;3.3 Dictionary Update&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/10.JPG&quot; alt=&quot;Dictionary update&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The procedure does not require to store all the vectors $x_i$ and $\alpha_i$, but only $A_t$ and $B_t$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sequentially updates each column of $D$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Orthogonal projection on to $C$(the constraint set)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, the matrix $A_t$ are often &lt;strong&gt;&lt;em&gt;concentrated on the diagonal&lt;/em&gt;&lt;/strong&gt;, which makes the block-coordinate descent more efficient&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;optimizing-the-algorithm&quot;&gt;3.4 Optimizing the Algorithm&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;3.4.1&lt;/em&gt;&lt;/strong&gt; Handling fixed-size data sets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Problem:&lt;/em&gt;&lt;/strong&gt; When the data sets are predefined and have finite size, the same data points may be examined several times&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Solution:&lt;/em&gt;&lt;/strong&gt; When the training set is small enough, it is possible to further speed up convergence:&lt;/p&gt;

        &lt;p&gt;If the sample $x$ has been drawn from the data sets twice in the iteration $t_0$ and $t$, we will replace $\alpha_{t_0}$ by $\alpha_t$ in $A_t$ and $B_t$, that is&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t = A_{t-1} + \alpha_t\alpha_t^T - \alpha_{t_0}\alpha_{t_0}^T&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;B_t = B_{t-1} + x_t\alpha_t^T - x_t\alpha_{t_0}^T&lt;/script&gt;

        &lt;p&gt;In this setting, we need to store $\alpha_i$ in every iteration and it’s &lt;strong&gt;&lt;em&gt;impractical&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;However, we can sovle this by removing the information from $A_t$ and $B_t$ that is older than two epochs(cycles through the data)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;3.4.2&lt;/em&gt;&lt;/strong&gt; Scaling te past data&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Problem:&lt;/em&gt;&lt;/strong&gt; At each iteration, the “new” information $\alpha_t$ that is added to $A_t$ and $B_t$ has the same weight as the “old” one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;Solution:&lt;/em&gt;&lt;/strong&gt; Rescaling the “old” information so that newer coefficients $\alpha_t$ have more weight, which is classical in online learning&lt;/p&gt;

        &lt;p&gt;We propose to replace lines 5 and 6 of Algorithm 1 by:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t = \beta_t A_{t-1} + \alpha_t\alpha_t^T&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;B_t = \beta_t B_{t-1} + x_t\alpha_t^T&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\beta_t = (1-\frac{1}{t})^\rho&lt;/script&gt;

        &lt;p&gt;In this circumstance, we propose&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;D_t = argmin_{D \in C}\frac{1}{\sum_{j=1}^t(j/t)} \sum_{i=1}^t(\frac{i}{t})^\rho(\frac{1}{2}\|x_i-D\alpha_i\|_2^2+\lambda\|\alpha_i\|_1)&lt;/script&gt;

        &lt;p&gt;=&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;argmin_{D \in C}\frac{1}{\sum_{j=1}^t(j/t)}(\frac{1}{2}Tr(D^TDA_t)-Tr(D^TB_t))&lt;/script&gt;

        &lt;p&gt;When $\rho = 0$, we obtain the original version of the algorithm&lt;/p&gt;

        &lt;p&gt;&lt;strong&gt;&lt;em&gt;In practice, this parameter $\rho$ is only useful for large data sets $(n \ge 100000)$&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;3.4.3&lt;/em&gt;&lt;/strong&gt; Mini-batch extension&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The complexity of computing $\eta$ vectors $\alpha_i$ is not linear in $\eta$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A &lt;strong&gt;&lt;em&gt;Cholesky-based implementation of LARS-Lasso&lt;/em&gt;&lt;/strong&gt; for decomposing one signal has a complexity of $O(kms+ks^2)$, where $s$ is the number of nonzero coefficient&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;When decomposing $\eta$ signals, it is possible to &lt;strong&gt;&lt;em&gt;pre-compute the Gram matrix $D^T_tD_t$&lt;/em&gt;&lt;/strong&gt; and the total complexity becomes $O(k^2m+\eta(km+ks^2))$, which is much cheaper than $\eta$ times the previous complexity when $\eta$ is large enough and $s$ is small&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;We propose to replace lines 5 and 6 of Algorithm 1 by:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t = A_{t-1} + \frac{1}{\eta}\sum_{i=1}^\eta \alpha_{t,i} \alpha_{t,i}^T&lt;/script&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;B_t = B_{t-1} + \frac{1}{\eta}\sum_{i=1}^\eta x_{t,i}\alpha_{t,i}^T&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;3.4.4&lt;/em&gt;&lt;/strong&gt; Slowing down the first iterations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Problem:&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;The first iterations of our algorithm may update the parameters with large steps, immediately leading to large deviations from the initial dictionary&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;solution:&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Use gradient steps of the form $\frac{a}{b+t}$&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;$b$ will slow down the first few steps&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;An initialization of the form $A_0 = t_0I$ and $B_0 = t_0 D_0$ with $t_0 \ge 0$ will also slow down the first steps&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;3.4.5&lt;/em&gt;&lt;/strong&gt; Puring the dictionary from unused atoms&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Problem:&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Some of the dictionary atoms are never (or very seldom) used, which typically happens with a very bad initialization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;solution:&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;In general cases, replacing these atoms during the optimization by randomly chosen elements of the training set&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For more difficult and highly regularized cases, choosing a continuation strategy consisting of starting from an easier, less regularized problem, and gradually increasing $\lambda$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;link-with-second-order-stochastic-gradient-descent&quot;&gt;3.5 Link with Second-order Stochastic Gradient Descent&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;convergence-analysis&quot;&gt;4. Convergence Analysis&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;extensions-to-matrix-factorization&quot;&gt;5. Extensions to Matrix Factorization&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;using-different-regularizers-for-alpha&quot;&gt;5.1 Using Different Regularizers for $\alpha$&lt;/h4&gt;

&lt;p&gt;Different priors for the coefficients $\alpha$ may lead to different regularizers $\Psi(\alpha)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Positivity constraints on $\alpha$ that are added to the $l_1-regularization$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Tikhonov regularization $\Psi(\alpha) = \frac{\lambda_1}{2}|\alpha|_2^2$, which doesn’t lead to sparse solutions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The elastic net $\Psi(\alpha) = \lambda_1|\alpha|_1 + \frac{\lambda_2}{2}|\alpha|_2^2$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The group Lasso $\Psi(\alpha) = \sum_{i=1}^s|\alpha|_2$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is &lt;strong&gt;&lt;em&gt;no theoretical convergence results in exploiting non-convex regularizers&lt;/em&gt;&lt;/strong&gt; such as $l_0$ pseduo-norm and $l_p$ pseduo-norm with $p &amp;lt; 1$&lt;/p&gt;

&lt;h4 id=&quot;using-different-constraint-sets-for-d&quot;&gt;5.2 Using Different Constraint Sets for $D$&lt;/h4&gt;

&lt;p&gt;In dictionary learning, we use an $l_2$-regularization on $D$ by forcing its columns to have less than unit $l_2-norm$, and thus the dictionary update step can be solved efficiently using a block-coordinate descent approach&lt;/p&gt;

&lt;p&gt;We can use different convex constraint sets $C′$ as long as the constraints are a union of independent constraints on each column of $D$ and the orthogonal projections of the vectors $u_j$ onto $C′$ can be done efficiently&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The “non-negative” constraints&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \{D \in R^{m * k}\quad s.t. \quad \forall j=1, ... ,k \quad \|d_j\|_2 \le 1 \quad and \quad d_j \ge 0\}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The “elastic-net” constraints&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \{D \in R^{m * k}\quad s.t. \quad \forall j=1, ... ,k \quad \|d_j\|_2^2+\gamma\|d_j\|_1 \le 1\}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using the $l_1-norm$ only in such problems lead to trivial solutions when $k$ is large enough&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “fused lasso” constraints&lt;/p&gt;

    &lt;p&gt;This kind of regularization has proven to be useful for &lt;strong&gt;&lt;em&gt;exploiting genomic data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \{D \in R^{m * k}\quad s.t. \quad \forall j=1, ... ,k \quad \|d_j\|_2^2 + \gamma_1\|d_j\|_1 + \gamma_2FL(d_j) \le 1\}&lt;/script&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;FL(u) = \sum_{i=2}^m \|u[i]-u[i-1]\|&lt;/script&gt; which is the $l_1-norm$ of the consecutive differences of $u$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;&lt;em&gt;orthogonal projection onto the “non negative” ball&lt;/em&gt;&lt;/strong&gt; is simple (additional thresholding)&lt;/p&gt;

&lt;p&gt;But &lt;strong&gt;&lt;em&gt;the projection onto the two other sets&lt;/em&gt;&lt;/strong&gt; is slightly more involved, however they can also be done efficently using some algorithms&lt;/p&gt;

&lt;h4 id=&quot;non-negative-matrix-factorization-nmf&quot;&gt;5.3 Non Negative Matrix Factorization (NMF)&lt;/h4&gt;

&lt;h4 id=&quot;sparse-principal-component-analysis-spca&quot;&gt;5.4 Sparse Principal Component Analysis (SPCA)&lt;/h4&gt;

&lt;h4 id=&quot;constrained-sparse-coding&quot;&gt;5.5 Constrained Sparse Coding&lt;/h4&gt;

&lt;h4 id=&quot;simultaneous-sparse-coding&quot;&gt;5.6 Simultaneous Sparse Coding&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experimental-validation&quot;&gt;6. Experimental Validation&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>icbcbicc</name></author><summary>Online Learning for Matrix Factorization and Sparse Coding
Authors: Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro
PDF</summary></entry></feed>
